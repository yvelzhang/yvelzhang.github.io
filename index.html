<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="喵喵喵">
<meta property="og:type" content="website">
<meta property="og:title" content="yvelzhang">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="yvelzhang">
<meta property="og:description" content="喵喵喵">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="yvelzhang">
<meta name="twitter:description" content="喵喵喵">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>yvelzhang</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">yvelzhang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/GBDT原理解读/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/GBDT原理解读/" itemprop="url">GBDT原理解读</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T16:12:01+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考：<a href="https://blog.csdn.net/zpalyq110/article/details/79527653" target="_blank" rel="noopener">https://blog.csdn.net/zpalyq110/article/details/79527653</a></p>
<p>GBDT论文：<a href="http://biostat.jhsph.edu/~mmccall/articles/friedman_1999.pdf" target="_blank" rel="noopener">http://biostat.jhsph.edu/~mmccall/articles/friedman_1999.pdf</a></p>
<p>GBDT使用的决策树是CART回归树（无论是处理回归还是二分类），原因是GBDT每次拟合的都是梯度值（连续值），因此要使用回归树。回归树寻找最佳二分点使用的是平方误差，而不是基尼指数或熵。</p>
<h2 id="回归树生成原理"><a href="#回归树生成原理" class="headerlink" title="回归树生成原理"></a><strong>回归树生成原理</strong></h2><p>在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：</p>
<p>（1）选择最优切分变量j与切分点s，求解</p>
<img src="/2020/03/20/GBDT原理解读/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/GBDT原理解读/image-20200320164209194.png" alt="image-20200320164209194" style="zoom:50%;">

<p>遍历变量j，对固定的切分变量j扫描切分点s，选择使得上式达到最小值的对(j,s)。其中R1, R2是（j,s）分割出来的两个输入空间，c1、c2分别是这两个空间的输出均值。</p>
<p>（2）用选定的对(j,s)划分区域并决定相应的输出值：</p>
<img src="/2020/03/20/GBDT原理解读/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/GBDT原理解读/image-20200320164629727.png" alt="image-20200320164629727" style="zoom:50%;">

<p>（3）继续对两个子区域调用步骤（1）和（2），直至满足停止条件</p>
<p>停止条件：</p>
<p>a . 叶子节点的数据y相同</p>
<p>b. 叶子节点样本个数小于阈值</p>
<p>c. 到达深度上限，叶子个数上限</p>
<p>（4）将输入空间划分为M个区域 R1,R2,…RM，生成决策树：</p>
<img src="/2020/03/20/GBDT原理解读/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/GBDT原理解读/image-20200320164734682.png" alt="image-20200320164734682" style="zoom:50%;">

<p>此方法的复杂度较高，尤其在每次寻找切分点时，需要遍历当前所有特征的所有可能取值，假如总共有F个特征，每个特征有N个取值，生成的决策树有S个内部节点，则该算法的时间复杂度为：O(F x N x S)</p>
<h2 id="Gradient-Boosting：-拟合负梯度"><a href="#Gradient-Boosting：-拟合负梯度" class="headerlink" title="Gradient Boosting： 拟合负梯度"></a>Gradient Boosting： 拟合负梯度</h2><p>梯度提升树（Grandient Boosting）是提升树（Boosting Tree）的一种改进算法，所以在讲梯度提升树之前先来说一下提升树</p>
<p>通俗举例：假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果</p>
<p><img src="/2020/03/20/GBDT原理解读/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/GBDT%E5%8E%9F%E7%90%86%E8%A7%A3%E8%AF%BB/image-20200321144903747.png" alt="image-20200321144903747"></p>
<p><strong>残差是什么</strong></p>
<img src="/2020/03/20/GBDT原理解读/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/GBDT原理解读/image-20200321150044162.png" alt="image-20200321150044162" style="zoom:50%;">

<p>当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Friedman提出了梯度提升树算法，这是利用最速下降的近似方法，<strong>其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。</strong></p>
<p><strong>什么是负梯度？</strong></p>
<img src="/2020/03/20/GBDT原理解读/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/GBDT原理解读/image-20200321150816821.png" alt="image-20200321150816821" style="zoom:50%;">

<h2 id="GBDT算法流程"><a href="#GBDT算法流程" class="headerlink" title="GBDT算法流程"></a>GBDT算法流程</h2><p><img src="/2020/03/20/GBDT原理解读/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/GBDT%E5%8E%9F%E7%90%86%E8%A7%A3%E8%AF%BB/image-20200321155314678.png" alt="image-20200321155314678"></p>
<h2 id="GBDT的优缺点"><a href="#GBDT的优缺点" class="headerlink" title="GBDT的优缺点"></a>GBDT的优缺点</h2><p><strong>优点：</strong></p>
<ul>
<li>可以灵活处理各种类型的数据，包括连续值和离散值。</li>
<li>在相对少的调参时间情况下，预测的精度高。这个是相对SVM来说的。</li>
<li>使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>由于弱学习器之间存在依赖关系，难以并行训练数据</li>
<li>如果数据维度较高时会加大算法的计算复杂度</li>
</ul>
<h2 id="其他知识点"><a href="#其他知识点" class="headerlink" title="其他知识点"></a>其他知识点</h2><p><strong>学习率</strong></p>
<p>Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。因此每次叠加新的一棵树时，会乘一个学习率，学习率通过线性搜索获得，公式如下：</p>
<img src="/2020/03/20/GBDT原理解读/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/GBDT原理解读/image-20200321163345096.png" alt="image-20200321163345096" style="zoom:50%;">

<p><strong>提升树和梯度提升树的区别</strong></p>
<ol>
<li>提升树初始化标签为0，GBDT初始化标签为使全局损失函数最小的值。</li>
<li>当损失函数比较简单时（平方损失，指数损失），提升树残差容易计算。对于一般损失函数而言，GBDT使用损失函数的负梯度来近似残差。<strong>残差是能使该样本在本轮损失函数最小的值</strong>（可以回看前面的残差环节）。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/lightgbm原理解读/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/lightgbm原理解读/" itemprop="url">lightgbm原理解读</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T16:05:39+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>论文：<a href="http://103.95.217.77/papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf" target="_blank" rel="noopener">http://103.95.217.77/papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf</a></p>
<p>源码：<a href="https://github.com/microsoft/LightGBM" target="_blank" rel="noopener">https://github.com/microsoft/LightGBM</a></p>
<p>GBDT原理解读：</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/11/回归问题中的各种损失函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/11/回归问题中的各种损失函数/" itemprop="url">回归问题常用的各种损失函数和评估指标</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-11T14:46:31+08:00">
                2020-03-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>损失函数：L1，L2，huber，log-cosh，quantile，gamma</p>
<p>评估指标：mse，rmse，mae，mape</p>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="L1（又称MAE）"><a href="#L1（又称MAE）" class="headerlink" title="L1（又称MAE）"></a>L1（又称MAE）</h2><img src="/2020/03/11/回归问题中的各种损失函数/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/回归问题中的各种损失函数/image-20200311145739046.png" alt="image-20200311145739046" style="zoom:50%;">

<p>如果对所有样本点只给出一个预测值，那么这个值就是所有目标值的中位数。<br>优点：</p>
<ul>
<li>对异常值具有较好鲁棒性</li>
</ul>
<p>缺点：</p>
<ul>
<li>梯度不变是个严重问题，即使对于很小的损失，梯度也很大，不利于模型收敛（比如神经网络），常使用变化的学习率解决，或者使用huber损失</li>
</ul>
<h2 id="L2（又称MSE）"><a href="#L2（又称MSE）" class="headerlink" title="L2（又称MSE）"></a>L2（又称MSE）</h2><img src="/2020/03/11/回归问题中的各种损失函数/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/回归问题中的各种损失函数/image-20200311145816310.png" alt="image-20200311145816310" style="zoom:50%;">

<p>回归问题中最常见的损失函数。如果对所有样本点只给出一个预测值，那么这个值就是所有目标值的平均值。<br>优点：</p>
<ul>
<li>计算方便，逻辑清晰，衡量误差较准确</li>
<li>梯度随着误差增大或减小，收敛效果好</li>
</ul>
<p>缺点：</p>
<ul>
<li>对异常点(离群值)敏感，异常点会产生较大的损失，模型追求对异常点预估准确从而牺牲整体精度。特别是对于长尾分布的数据（小数值占一大部分，同时会有长尾的大数值），长尾数据产生较大的loss，导致模型过于关注少量长尾数据，从而对于其他数据预估精度降低。</li>
</ul>
<p>例如某用户付费数据分布，大量用户付费在0附近，但是也会有较长的尾部数据，对于这种数据集使用L2，会导致模型倾向于准确预估高付费用户，低付费用户预估精度低，因此推荐使用L1损失：</p>
<img src="/2020/03/11/回归问题中的各种损失函数/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/回归问题中的各种损失函数/image-20200311155538130.png" alt="image-20200311155538130" style="zoom: 33%;">

<p><strong>L1和L2对比：</strong>对异常值而言，中位数比均值更加鲁棒，因此MAE对于异常值也比MSE更稳定</p>
<h2 id="huber"><a href="#huber" class="headerlink" title="huber"></a>huber</h2><img src="/2020/03/11/回归问题中的各种损失函数/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/回归问题中的各种损失函数/image-20200311161547867.png" alt="image-20200311161547867" style="zoom:50%;">

<img src="/2020/03/11/回归问题中的各种损失函数/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/回归问题中的各种损失函数/image-20200311161646504.png" alt="image-20200311161646504" style="zoom: 33%;">

<p>当误差在[0-δ,0+δ]之间时，等价为MSE，而在[-∞,δ]和[δ,+∞]时为MAE<br>优点：</p>
<ul>
<li>对异常值更加鲁棒</li>
<li>在最优点附近由于调整为MSE，梯度更新会随着误差减小而减小，有利于收敛</li>
</ul>
<p>缺点：</p>
<ul>
<li>引入额外的超参，需要调试</li>
<li>临界点delta处不可导</li>
</ul>
<h2 id="log-cosh"><a href="#log-cosh" class="headerlink" title="log-cosh"></a>log-cosh</h2><img src="/Users/yifeizhang/Library/Application Support/typora-user-images/image-20200311163244042.png" alt="image-20200311163244042" style="zoom:50%;">

<p>比MSE更加平滑的损失函数，对于较小的x，log(cosh(x))近似等于(x^2)/2，对于较大的x，近似等于abs(x)-log(2)。这意味着log-cosh基本类似于均方误差，但不易受到异常点的影响。它具有Huber损失所有的优点，但不同于Huber损失的是，log-cosh二阶处处可微。。<br>优点：</p>
<ul>
<li>具有huber损失具备的所有优点（不受异常点影响，处处可微）</li>
<li>二阶处处可微，许多机器学习算法采用牛顿法逼近最优点，比如鼎鼎大名的XGBoost算法，而牛顿法要求损失函数二阶可微。</li>
</ul>
<p>缺点：</p>
<ul>
<li>误差很大情况下，一阶梯度和Hessian会变成定值，导致XGBoost出现缺少分裂点的情况。</li>
</ul>
<p>二阶可微的好处：凸优化求最小值的充要条件是一阶导数为0，二阶导数＞0。</p>
<h3 id="Quantile-Loss（分位数损失）"><a href="#Quantile-Loss（分位数损失）" class="headerlink" title="Quantile Loss（分位数损失）"></a>Quantile Loss（分位数损失）</h3><p>通常的回归算法是拟合训练数据的期望或者中位数，而使用分位数损失函数可以通过给定不同的分位点，拟合训练数据的不同分位数。分位数回归提出的原因，就是因为不希望仅仅是研究y的期望，而是希望能探索y的<strong>完整分布</strong>状况，或者说可能在某些情况下我们更希望了解y在给定x情况下的某个分位数</p>
<img src="/2020/03/11/回归问题中的各种损失函数/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/回归问题中的各种损失函数/image-20200311165930000.png" alt="image-20200311165930000" style="zoom: 50%;">

<img src="/2020/03/11/回归问题中的各种损失函数/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/回归问题中的各种损失函数/image-20200311170137928.png" alt="image-20200311170137928" style="zoom:50%;">

<p>该函数是一个分段函数，𝛾为分位数系数，𝑦为真实值，𝑓(𝑥)为预测值。根据预测值和真实值的大小，分两种情况来开考虑。𝑦&gt;𝑓(𝑥)为高估，预测值比真实值大；𝑦&lt;𝑓(𝑥)为低估，预测值比真实值小，<strong>使用不同过得系数来控制高估和低估在整个损失值的权重</strong> 。</p>
<p>特别的，当𝛾=0.5时，分位数损失退化为平均绝对误差MAE，也可以将MAE看成是分位数损失的一个特例 - 中位数损失。下图是取不同的中位点[0.25,0.5,0.7]得到不同的分位数损失函数的曲线，也可以看出0.5时就是MAE。</p>
<img src="/2020/03/11/回归问题中的各种损失函数/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/回归问题中的各种损失函数/image-20200311170441005.png" alt="image-20200311170441005" style="zoom: 33%;">

<h1 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h1><h2 id="mse，rmse"><a href="#mse，rmse" class="headerlink" title="mse，rmse"></a>mse，rmse</h2><p>MSE: 等效于L2损失，用MSE来评估算法精度时会对离群值敏感，追求对离群值预估准确从而牺牲整体精度，公式如下：</p>
<img src="/Users/yifeizhang/Library/Application Support/typora-user-images/image-20200311171821899.png" alt="image-20200311171821899" style="zoom:25%;">

<p>RMSE：MSE开根号，本质上和MSE没区别，只是为了数量级与真实数据相同，更容易理解。</p>
<h2 id="mae"><a href="#mae" class="headerlink" title="mae"></a>mae</h2><img src="/2020/03/11/回归问题中的各种损失函数/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/回归问题中的各种损失函数/image-20200311172144716.png" alt="image-20200311172144716" style="zoom:25%;">

<p>等效于L1损失，用该值评估算法精度能解决MSE对离群值敏感的问题。</p>
<h2 id="mape"><a href="#mape" class="headerlink" title="mape"></a>mape</h2><p>平均绝对百分比误差（Mean Absolute Percentage Error）</p>
<img src="/2020/03/11/回归问题中的各种损失函数/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/回归问题中的各种损失函数/image-20200311172900880.png" alt="image-20200311172900880" style="zoom:50%;">

<h2 id="R2"><a href="#R2" class="headerlink" title="R2"></a>R2</h2><img src="/2020/03/11/回归问题中的各种损失函数/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/回归问题中的各种损失函数/image-20200311184606276.png" alt="image-20200311184606276" style="zoom:33%;">

<p>其中，分子部分表示真实值与预测值的平方差之和，类似于均方差 MSE；分母部分表示真实值与均值的平方差之和，类似于方差 Var。根据 R-Squared 的取值，来判断模型的好坏，其取值范围为[0,1]：</p>
<p>如果结果是 0，说明模型拟合效果很差；如果结果是 1，说明模型无错误。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/17/模型融合blending和stack/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/17/模型融合blending和stack/" itemprop="url">集成学习blending和stacking</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-17T16:32:53+08:00">
                2020-02-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>集成学习主要范围如下，本篇主要讲blending和stacking：</p>
<img src="/2020/02/17/模型融合blending和stack/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/模型融合blending和stack/image-20200217163456322.png" alt="image-20200217163456322" style="zoom:50%;">

<h3 id="1-blending"><a href="#1-blending" class="headerlink" title="1.blending"></a>1.blending</h3><p>（1）uniform blending</p>
<p>最终结果是各个子学习器结果的投票（分类问题）或平均（回归问题），各个子学习器的结果具有相同权重。</p>
<p>（2）linear blending</p>
<p><img src="/2020/02/17/模型融合blending和stack/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88blending%E5%92%8Cstack/image-20200217164808032.png" alt="image-20200217164808032"></p>
<p>最终结果是各个子学习器（上图中的M1 M2…MM）结果的加权叠加，各个子学习器的结果具有不同权重，通过再训练一个线性模型（上图中的M0）来学习各个子模型的权重（子模型的输出即为第二层线性模型的输入特征）</p>
<p><strong>将原始的训练数据集按照一定的比例分为训练数据集DT和验证(或者说测试)数据集DA，算法训练过程如下：</strong></p>
<p>假设要构建M个模型M1，M2，……MM。以模型Mi为例说明：对训练数据集DT进行学习，得到模型Mi。学习完毕后对验证数据DA的计算结果为DA_Mi，对预测数据集合的计算结果为DP_Mi。待M个模型全部建立完成后：</p>
<ol>
<li>对验证数据得到的结果集合为DA_M1，DA_M2，……，DA_MM，其中每个结果序列都可看作一个新的特征，将这些新的特征排列起来看作一个训练数据集的输入，输出就是原始数据集中验证数据集的输出。输出和输入结合起来构成一个完整的训练数据集；</li>
<li>对预测数据得到的结果集合为DP_M1，DP_M2，……，DP_MM，其中每个结果序列都可看作一个新的特征，将这些新的特征排列起来看作一个预测数据集的输入；</li>
<li>对a中得到的训练数据集建立模型M0，学习完毕后，该模型对于b中得到的预测数据集进行计算，得到最终的结果；</li>
</ol>
<p>（3）any blending</p>
<p>与linear blending相似，唯一不同是子模型的权重可以由任何模型（不一定要是线性模型）来进行训练。</p>
<h3 id="2-stacking"><a href="#2-stacking" class="headerlink" title="2.stacking"></a>2.stacking</h3><p>代码参考（<a href="https://blog.csdn.net/zgj_gutou/article/details/88598934）" target="_blank" rel="noopener">https://blog.csdn.net/zgj_gutou/article/details/88598934）</a></p>
<p>结构示意如下：</p>
<img src="/2020/02/17/模型融合blending和stack/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/模型融合blending和stack/image-20200217195553120.png" alt="image-20200217195553120" style="zoom: 50%;">

<p>这里我们用二分类的例子做介绍。<br>例如我们用 RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier 作为第一层学习器（当然这里我们可以添加更多的分类器，也可以用不同的特征组合但是同样的学习方法作为基分类器）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clfs = [</span><br><span class="line">        RandomForestClassifier(n_estimators = n_trees, criterion = <span class="string">'gini'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators = n_trees * 2, criterion = <span class="string">'gini'</span>),</span><br><span class="line">        GradientBoostingClassifier(n_estimators = n_trees),</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure>

<p>step1:** 训练第一层学习器，并得到第二层学习器所需要的数据，这里会用到 k 折交叉验证。我们首先会将数据集进行一个划分，比如使用80%的训练数据来训练，20%的数据用来测试</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dev_cutoff = len(Y) * <span class="number">4</span>/<span class="number">5</span></span><br><span class="line">    X_dev = X[<span class="symbol">:dev_cutoff</span>]</span><br><span class="line">    Y_dev = Y[<span class="symbol">:dev_cutoff</span>]</span><br><span class="line">    X_test = X[<span class="symbol">dev_cutoff:</span>]</span><br><span class="line">    Y_test = Y[<span class="symbol">dev_cutoff:</span>]</span><br></pre></td></tr></table></figure>

<p>然后对训练数据通过交叉验证训练 clf，并得到第二层的训练数据 blend_train,同时，在每个基分类器的每一折交叉验证中，我们都会对测试数据进行一次预测，以得到我们blend_test，二者的定义如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blend_train = np.zeros((X_dev.shape[0], len(clfs))) # Number of training data x Number of classifiers</span><br><span class="line">blend_test = np.zeros((X_test.shape[0], len(clfs))) # Number of testing data x Number of classifiers</span><br></pre></td></tr></table></figure>

<p>按照上面说的，blend_train基于下面的方法得到，注意，下图是对于一个分类器来说的，所以每个分类器得到的blend_train的行数与用于训练的数据一样多，所以blend_train的shape为X_dev.shape[0]*len(clfs)，即训练集长度 * 基分类器个数：</p>
<p><img src="/2020/02/17/模型融合blending和stack/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88blending%E5%92%8Cstack/image-20200217200632015.png" alt="image-20200217200632015"></p>
<p>而对于第二轮的测试集blend_test来说，由于每次交叉验证的过程中都要进行一次预测，假设我们是5折交叉验证，那么对于每个分类器来说，得到的blend_test的shape是测试集行数 * 交叉验证折数，此时的做法是，对axis=1方向取平均值，以得到测试集行数 * 1 的测试数据，所以总的blend_test就是测试集行数 * 基分类器个数，可以跟blend_train保持一致：</p>
<img src="/2020/02/17/模型融合blending和stack/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/模型融合blending和stack/image-20200217200656391.png" alt="image-20200217200656391" style="zoom:50%;">

<p>得到blend_train 和 blend_test的代码如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> enumerate(clfs):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">'Training classifier [%s]'</span> % (j)</span><br><span class="line">        blend_test_j = np.zeros((X_test.shape[0], len(skf))) <span class="comment"># Number of testing data x Number of folds , we will take the mean of the predictions later</span></span><br><span class="line">        <span class="keyword">for</span> i, (train_index, cv_index) <span class="keyword">in</span> enumerate(skf):</span><br><span class="line">            <span class="built_in">print</span> <span class="string">'Fold [%s]'</span> % (i)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># This is the training and validation set</span></span><br><span class="line">            X_train = X_dev[train_index]</span><br><span class="line">            Y_train = Y_dev[train_index]</span><br><span class="line">            X_cv = X_dev[cv_index]</span><br><span class="line">            Y_cv = Y_dev[cv_index]</span><br><span class="line">            </span><br><span class="line">            clf.fit(X_train, Y_train)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># This output will be the basis for our blended classifier to train against,</span></span><br><span class="line">            <span class="comment"># which is also the output of our classifiers</span></span><br><span class="line">            blend_train[cv_index, j] = clf.predict(X_cv)</span><br><span class="line">            blend_test_j[:, i] = clf.predict(X_test)</span><br><span class="line">        <span class="comment"># Take the mean of the predictions of the cross validation set</span></span><br><span class="line">        blend_test[:, j] = blend_test_j.mean(1)</span><br></pre></td></tr></table></figure>

<p>接着我们就可以用 blend_train, Y_dev 去训练第二层的学习器 LogisticRegression(当然也可以是别的分类器，比如lightGBM，XGBoost）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bclf = LogisticRegression()</span><br><span class="line">bclf.fit(blend_train, Y_dev)</span><br></pre></td></tr></table></figure>

<p>最后，基于我们训练的二级分类器，我们可以预测测试集 blend_test，并得到 score：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Y_test_predict = bclf.predict(blend_test)</span><br><span class="line">score = metrics.accuracy_score(Y_test, Y_test_predict)</span><br><span class="line"><span class="built_in">print</span> <span class="string">'Accuracy = %s'</span> % (score)</span><br></pre></td></tr></table></figure>

<p>如果是多分类怎么办呢，我们这里就不能用predict方法，我么要用的是predict_proba方法，得到基分类器对每个类的预测概率代入二级分类器中训练，修改的部分代码如下：</p>
<figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">blend_train = np.zeros((np.<span class="keyword">array</span>(X_dev.values.tolist()).shape[<span class="number">0</span>], num_classes*len(clfs)),dtype=np.float32)  <span class="comment"># Number of training data x Number of classifiers</span></span><br><span class="line">blend_test = np.zeros((np.<span class="keyword">array</span>(X_test.values.tolist()).shape[<span class="number">0</span>], num_classes*len(clfs)),dtype=np.float32)  <span class="comment"># Number of testing data x Number of classifiers</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># For each classifier, we train the number of fold times (=len(skf))</span></span><br><span class="line">    <span class="keyword">for</span> j, clf in enumerate(clfs):</span><br><span class="line">        <span class="keyword">for</span> i, (train_index, cv_index) in enumerate(skf):</span><br><span class="line">            <span class="keyword">print</span>(<span class="string">'Fold [%s]'</span> % (i))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># This is the training and validation set</span></span><br><span class="line">            X_train = X_dev[train_index]</span><br><span class="line">            Y_train = Y_dev[train_index]</span><br><span class="line">            X_cv = X_dev[cv_index]</span><br><span class="line"></span><br><span class="line">            X_train = np.concatenate((X_train, ret_x),axis=<span class="number">0</span>)</span><br><span class="line">            Y_train = np.concatenate((Y_train, ret_y),axis=<span class="number">0</span>)</span><br><span class="line">            clf.fit(X_train, Y_train)</span><br><span class="line">            blend_train[cv_index, j*num_classes:(j+<span class="number">1</span>)*num_classes] = clf.predict_proba(X_cv)</span><br><span class="line">            blend_test[:, j*num_classes:(j+<span class="number">1</span>)*num_classes] += clf.predict_proba(X_test)</span><br><span class="line">blend_test = blend_test / float(n_folds)</span><br></pre></td></tr></table></figure>

<p>上面的代码修改的主要就是blend_train和blend_test的shape，可以看到，对于多分类问题来说，二者的第二维的shape不再是基分类器的数量，而是class的数量*基分类器的数量，这是大家要注意的，否则可能不会得到我们想要的结果。</p>
<h3 id="3-Stacking-和-blending的区别"><a href="#3-Stacking-和-blending的区别" class="headerlink" title="3.Stacking 和 blending的区别"></a>3.Stacking 和 blending的区别</h3><p>bending和stacking的区别：Blending的主要区别在于训练集不是通过K-Fold的CV策略来获得预测值从而生成第二阶段模型的特征，而是建立一个Holdout集，例如10%的训练数据，第二阶段的stacker模型就基于第一阶段模型对这10%训练数据的预测值进行拟合</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/08/SEM笔记/未命名/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/08/SEM笔记/未命名/" itemprop="url">未命名</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-08T20:12:16+08:00">
                2019-11-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/01/spark知识点/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/01/spark知识点/" itemprop="url">spark知识点</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-01T20:12:42+08:00">
                2019-11-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="sparkCore（RDD）"><a href="#sparkCore（RDD）" class="headerlink" title="sparkCore（RDD）"></a>sparkCore（RDD）</h3><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a><strong>特点</strong></h4><p>分区：</p>
<p>数据是由多个分区组成</p>
<p>可以指定分区的方式</p>
<p>抽象：</p>
<p>每个分区不一定有物理存储</p>
<p>只能通过接口获取分区数据（只读：只能RDD-&gt;RDD）</p>
<h4 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h4><p>两类算子：transformations 和action<br>窄依赖：分区一对一<br>宽依赖：分区多对多，需shuffle</p>
<p><strong>Transformation算子：</strong></p>
<p>map/filter/flatmap/mapPartitions/mapPartitionsWithIndex/sample/union/intersection</p>
<p>distinct/groupByKey/reduceByKey/aggregateByKey/sortByKey/join/cogroup/cartesian/pipe</p>
<p>coalesce/repatition/repartitionAndSortWithinPartitions</p>
<p><strong>Action算子</strong></p>
<p>reduce/collect/count/first/takeSample/take/takeOrdered/saveAsTextFile/repartitionAndSortWithinPartitions</p>
<p>saveAsObjectFile/countByKey/foreach</p>
<p>详细应用参照链接 <a href="https://blog.csdn.net/Fortuna_i/article/details/81170565" target="_blank" rel="noopener">https://blog.csdn.net/Fortuna_i/article/details/81170565</a></p>
<h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><p>job：action为界<br>stage：job的子集，以shuffle为界（宽依赖）<br>task：stage的子集，根据分区数量为定</p>
<p>Stage调度执行：从最后一个stage反向回溯，寻找所依赖的stage</p>
<p>Task调度：开启一个线程循环，不断根据当前可用资源去取task执行</p>
<h4 id="资源配置"><a href="#资源配置" class="headerlink" title="资源配置"></a>资源配置</h4><p>stage的分区数量决定task有多少</p>
<p>单个executor有多个core，每个core运行一个task，单个executor多个task共享内存</p>
<p>建议避免：<br>太多的cpu或者太多的executor<br>太多分区（任务过细，轮数过多，单个core不超过10个partition）</p>
<h4 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h4><p>过程分为二阶段 write &amp; read<br>引发shuffle操作的算子：repartition/*bykey/join &amp; cogroup</p>
<p>代价很高（序列化，跨机器，读写文件）</p>
<p><strong>阶段一 write：</strong><br>datafile： 使用map分区（hash），每个分区内数据按照记录排序<br>index file： 索引文件</p>
<p><strong>阶段二 read：</strong><br>每个分区，根据索引文件，从相应datafile中读取数据，做归并排序</p>
<p><strong>shuffle调优</strong></p>
<p>a.优化数据结构：</p>
<p>尽可能使用原生类型（Int，Long，Double）</p>
<p>尽可能使用对象数组以及原生类型数组以替代Java或者scala集合类</p>
<p>尽可能避免采用潜逃数据结构保存小对象</p>
<p>b.主动shuffle repartition：</p>
<p>如果分区较少, 可加大分区，将任务细分</p>
<p>c.提高后续分布式运行的速度</p>
<p>调整shuffle read 并发（根据内存大小）：maxSizeInFlight参数</p>
<p>d.避免使用groupbykey （OOM风险，单个key连续内存） 可以用reducebykey/aggregateByKey优化</p>
<p>例子：groupByKey.mapValues(_.sum) </p>
<p>可以用rdd.reduceByKey(_ _+ _)来代替</p>
<h4 id="broadcast"><a href="#broadcast" class="headerlink" title="broadcast"></a>broadcast</h4><p>广播大对象（超过10M）：每个executor传输一次大对象，任务用到的时候直接本地取用，不需要每次都从driver传输</p>
<p>broadcast+map 替代 大表join小表：将小表broadcast，避免shuffle</p>
<p>解决数据倾斜join：将rdd分割成两部分进行join（大量数据的key单独分出来，进行broadcast+map，其他数据照常，最后union）</p>
<h4 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h4><p>persist原则 树形rdd分叉处persist<br>内存充足时 MEMORY_ONLY<br>稀缺时 MEMORY_ONLY_SER<br>谨慎使用 DISK_ONLY</p>
<p>unpersist原则 不使用时尽快unpersist</p>
<h4 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h4><p>截断rdd，并且保存到内存，提高链式rdd(依赖关系特别长)的可靠性</p>
<h3 id="sparkSQL"><a href="#sparkSQL" class="headerlink" title="sparkSQL"></a>sparkSQL</h3><p><strong>特点</strong></p>
<p>引入更高级的API（SQLtext  dataframe/dataset）<br>使用高级API时系统会自动优化，例如broadcast</p>
<p><strong>join</strong></p>
<p>inner join 大表小表前后顺序无所谓，底层会自动优化（大表为流失遍历表，小表为查找表）<br>left join 左表为流失遍历表，右表为查找表</p>
<p>关于Dataset常用API，以后再记录。</p>
<p><strong>官方API文档</strong></p>
<p><a href="http://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">http://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/sql/Dataset.html</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/25/SEM笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/25/SEM笔记/" itemprop="url">SEM笔记</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-25T15:46:06+08:00">
                2019-10-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>sem出价，广告展现排名根据出价的竞得排名来确定</p>
<p><strong>关键词价格调整策略</strong></p>
<ol>
<li>离成交近的词高出价</li>
<li>精确匹配高出价，宽匹配低出价</li>
<li>根据地域转化强弱出价</li>
<li>根据时段和终端，这个就是好的时段可以提高点出价，就是拉时段系数比例，另外PC和YD端也是要区分开来跑的</li>
<li>根据排名，转化率效果好的词，排名较低，可适当提价</li>
</ol>
<p>品牌词：防守策略，点击率高，一般出价高</p>
<p>竞品词：点击率低，一般出价低</p>
<p>类型词：点击率低，一般出价低</p>
<p><strong>百度现有出价策略：</strong></p>
<p>排名倾向：设定排名目标与出价范围，在范围内达成目标</p>
<p>排名胜出：设定对手url与出价范围，在范围内压制对手</p>
<p>点击最大化：在出价范围内，最大化点击</p>
<p><strong>缺点是无法和最终效果数据关联起来</strong></p>
<p><strong>应用宝sem策略：</strong></p>
<img src="/2019/10/25/SEM笔记/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/SEM笔记/image-20191028144300296.png" alt="image-20191028144300296" style="zoom: 25%;">

<p><strong>单个关键词收益最大化出价策略：</strong></p>
<p>边际收益&gt;0   寻找能拿到首位的最低出价</p>
<p>边际收益&lt;0   降低出价至边际收益大于0</p>
<p>（我们可以用CPA来替换其中的收益）</p>
<p><strong>拓词：</strong>基于NLP语义泛化拓词</p>
<p><strong>应用宝SEM闭环框架</strong>：</p>
<p><img src="/2019/10/25/SEM笔记/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/SEM%E7%AC%94%E8%AE%B0/image-20191028145940294.png" alt="image-20191028145940294"></p>
<p>我们的投放目标：在CPA可接受的条件下，尽可能多的带量</p>
<p>目标拆解：</p>
<p>a. 品牌词:  降成本</p>
<p>​    头部词：价格向下试探，保排名，保量；</p>
<p>​    非头部词（点击率低，出价高）：降价或关停，降低成本</p>
<p>b. 非品牌词： 提量（成本范围内）</p>
<p>​    头部词：向上提价，带更多量（发掘头部非品牌词）</p>
<p>​    非头部词（点击率低）：降价或关停，降低CPA成本</p>
<p>对无点击的词提高出价，使之积累一定的点击</p>
<p>利用点击数据试探对手的出价</p>
<h3 id="调价"><a href="#调价" class="headerlink" title="调价"></a>调价</h3><h4 id="品牌词：保排名，降低成本"><a href="#品牌词：保排名，降低成本" class="headerlink" title="品牌词：保排名，降低成本"></a><strong>品牌词：保排名，降低成本</strong></h4><p>设置：排名r，初始出价p0，设置出价上限pu，出价下限pd</p>
<p>输入参数：</p>
<p>​            品牌词ID</p>
<p>​            每个词前一日出价，展现/点击数据，平均点击价格，平均排名，所在计划，所在单元            </p>
<p>​            </p>
<p>每日调整，基于前一日数据：</p>
<p>for（every word）</p>
<p>{</p>
<p>​       if（排名 高于 r） 向下调价 0.2</p>
<p>​       if（排名 低于 r） 向上调价 min(  (排名 - r) * k， pu )，最低调价0.2</p>
<p>​       对于满足总曝光条件，点击率低于 1% 的品牌词，停止投放</p>
<p>}</p>
<h4 id="非品牌词（重点）：在目标CPA范围内，最大化转化"><a href="#非品牌词（重点）：在目标CPA范围内，最大化转化" class="headerlink" title="非品牌词（重点）：在目标CPA范围内，最大化转化"></a><strong>非品牌词（重点）：</strong>在目标CPA范围内，最大化转化</h4><p>根据目标CPA和单元CVR来算点击收益，控制新增点击成本&lt;新增点击收益</p>
<p>单元CVR：单元注册数量/单元点击数量</p>
<p>风险点：CVR数据稀疏</p>
<p>设置：初始出价p0，出价上限pu，出价下限pd，目标CPA</p>
<p>输入参数：非品牌词ID</p>
<p>​                   每个词前一日出价，展现/点击数据，平均点击价格，平均排名，所在计划，所在单元</p>
<p>​                   每个词累计展现/点击数据，平均点击价格，平均排名，所在计划，所在单元</p>
<p>​                   每个单元累计消费，展现，点击，注册/回流</p>
<p>每日调整，基于前一日数据,  以及累计数据：</p>
<p>for（every word）</p>
<p>{</p>
<p>​     if（累计点击 &lt; 20 且 排名低于1.1）向上调价 0.5     //冷启动          </p>
<p>​     if（累计点击&gt; 20 且 所在单元转化超过10个）</p>
<p>​     {</p>
<p>​          新增收益 = 前一日点击次数 * 单元cvr * 目标CPA</p>
<p>​          新增成本 = 前一日费用         </p>
<p>​          调价系数 = |（新增收益 - 新增成本）/  点击数量|</p>
<p>​           </p>
<p>​          //新增边际收益大于0，向上提价，寻找排名第一的最低价</p>
<p>​          //新增边际收益小于0，向下降价至边际收益大于等于0</p>
<p>​          If( 新增收益-新增成本&gt;0  and  排名低于1)    向上调价 0. 8  *  调价系数，最低调价 0. 2</p>
<p>​          If( 新增收益-新增成本 &gt;0 and  排名为1） 向下调价0.2试探底价           </p>
<p>​          if( 新增收益-新增成本 &lt;0 )    向下调价  1.2  *  调价系数，最低调价 0. 2         </p>
<p>​     }</p>
<p>}</p>
<h3 id="拓词"><a href="#拓词" class="headerlink" title="拓词"></a>拓词</h3><h4 id="品类词"><a href="#品类词" class="headerlink" title="品类词"></a>品类词</h4><p>品类相似度计算</p>
<h4 id="竟品词"><a href="#竟品词" class="headerlink" title="竟品词"></a>竟品词</h4><p>竟品相似度计算</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/21/ctr预估常用深度模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/21/ctr预估常用深度模型/" itemprop="url">ctr预估常用深度模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-21T08:55:00+08:00">
                2019-10-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>deepFM</p>
<p>NFM</p>
<p>AFM</p>
<p>wide&amp;deep</p>
<p>FNN</p>
<p>PNN</p>
<p>Deep cross</p>
<p>xdeepFM</p>
<p>Din</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/20/ctr预估常用模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/20/ctr预估常用模型/" itemprop="url">ctr预估常用现性模型（LR， FM， FFM）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-20T22:06:07+08:00">
                2019-10-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h2><p><strong>优点：</strong></p>
<p>实现容易，模型可解释</p>
<p><strong>缺点：</strong></p>
<p>只能学习线性关系</p>
<p>需要手动进行特征交叉（二阶，三阶交叉），工作量大，根据业务知识</p>
<p>特征工程工作量大，连续特征需要离散化</p>
<h2 id="FM-（2010年）"><a href="#FM-（2010年）" class="headerlink" title="FM （2010年）"></a>FM （2010年）</h2><p><strong>原理</strong>：</p>
<p>onehot的编码后特征极度稀疏，特征空间大。</p>
<p>通过多项式交叉后，交叉特征的非零样本更加稀疏；采用一种矩阵分解的思路，为每个单特征学习一个k维向量v（只要该非零特征出现了，v便能得到训练）。</p>
<p>复杂度线性/样本稀疏的情况下有优势。</p>
<p><strong>该模型公式如下：</strong></p>
<p><img src="/2019/10/20/ctr预估常用模型/yifeizhang/Desktop/zyf/hexo/blog/source/_posts/ctr%E9%A2%84%E4%BC%B0%E5%B8%B8%E7%94%A8%E6%A8%A1%E5%9E%8B/585228-20171123143833790-1275951688.png" alt="img"></p>
<h2 id="FMM-（2015年）"><a href="#FMM-（2015年）" class="headerlink" title="FMM （2015年）"></a>FMM （2015年）</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/24/shap模型解释/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/24/shap模型解释/" itemprop="url">shap模型解释</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-09-24T20:32:08+08:00">
                2019-09-24
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="模型可解释-SHAP"><a href="#模型可解释-SHAP" class="headerlink" title="模型可解释(SHAP)"></a>模型可解释(SHAP)</h1><h3 id="1-什么是shapley"><a href="#1-什么是shapley" class="headerlink" title="1. 什么是shapley"></a>1. 什么是shapley</h3><p>Shapley值由美国洛杉矶加州大学教授罗伊德·夏普利(Lloyd Shapley)提出，用于解决合作博弈的贡献<br>和收益分配问题。</p>
<p><strong>Shapley</strong>:单个成员所得与自己的贡献相等**</p>
<h3 id="2-shapley例子"><a href="#2-shapley例子" class="headerlink" title="2. shapley例子"></a>2. shapley例子</h3><p>来自百度百科:<a href="https://baike.baidu.com/item/Shapley" target="_blank" rel="noopener">https://baike.baidu.com/item/Shapley</a></p>
<h3 id="3-shapley满足四个公理"><a href="#3-shapley满足四个公理" class="headerlink" title="3. shapley满足四个公理"></a>3. shapley满足四个公理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">对称性:合作获利的分配不随每个人在合作中的记号或次序变化</span><br><span class="line">有效性:合作各方获利总和等于合作获利</span><br><span class="line">冗员性:如果一个成员在他参与的任何合作中都没有贡献，他不应该获利</span><br><span class="line">可加性:每种合作的利益分配方式与其他合作无关</span><br></pre></td></tr></table></figure>

<h3 id="4-shapley机器学习模型解释"><a href="#4-shapley机器学习模型解释" class="headerlink" title="4. shapley机器学习模型解释"></a>4. shapley机器学习模型解释</h3><p><img src="/2019/09/24/shap模型解释/image-20190924203704939.png" alt="image-20190924203704939"></p>
<p>基于Shap值的模型解释是一种和模型无关的方法。如图，模型预测和Shap值解释是两个并行流程，Shap对模型预测的结果进行解释。</p>
<h4 id="4-1-shap方法的两大特性"><a href="#4-1-shap方法的两大特性" class="headerlink" title="4.1 shap方法的两大特性"></a>4.1 shap方法的两大特性</h4><p><strong>（1）特征归因(收益)一致性</strong></p>
<p>定义：模型改变(A-&gt;B)，特征x的贡献不递减(增加或者保持现状)，则归因(收益)也不递减</p>
<p>特点：特征作用越大(小)，重要度越高(低),和模型变化无关</p>
<p>例子：我们构造了两个简单的树模型，样本数量2000，两维特征Fever和cough。两个树模型的样本数据<br>如下:</p>
<p><img src="/2019/09/24/shap模型解释/image-20190924203942877.png" alt="image-20190924203942877"></p>
<p>模型A. Fever=1且Cough=1的情况下，生病的可能性为80<br>模型B. Fever=1且Cough=1的情况下，生病的可能性为90;或者cough=1生病的可能性为10</p>
<p>可以看到，相比于Fever特征， Cough特征和生病相关性更强，Cough的贡献比Fever的贡献高。这里<br>选择了三种方法:Gain 、Split count、Permutation[7]。</p>
<p><img src="/2019/09/24/shap模型解释/image-20190924204016491.png" alt="image-20190924204016491"></p>
<p>全局特征一致性 ：</p>
<p>   mean(|Tree SHAP|): Shap值<br>   Gain : 特征用于划分时所带来的训练损失减益的平均值<br>   Split Count: 根据特征用于划分的次数计算重要性<br>   Permutation: 将特征的值随机排列，用排列前后的模型误差来计算重要性 </p>
<p><img src="/2019/09/24/shap模型解释/image-20190924204050105.png" alt="image-20190924204050105"></p>
<p>局部样本(Fever=yes,cough=yes的样本)一致性 ：</p>
<p>   Saabas : 树创建完成后,根据样本预测值，将父节点和子节点value的差异，作为父节点的 特征重要性<br>   Tree SHAP : 基于Shap值矩阵(样本数*特征数)，计算出Fever和Cough的重要性 </p>
<p><img src="/2019/09/24/shap模型解释/image-20190924204125367.png" alt="image-20190924204125367"></p>
<p><strong>结论: Shap方法和Permutation方法具有特征归因一致性</strong></p>
<p><strong>（2）特征归因(收益)可加性</strong></p>
<p>解释性方法如果具有<strong>特征归因可加性(Additive Feature Attribution Methods)</strong>，特征重要性和模<br>型预测值可以通过特征贡献的线性组合来表示。</p>
<p><img src="/2019/09/24/shap模型解释/image-20190924204336388.png" alt="image-20190924204336388"></p>
<p>满足三个性质:</p>
<p><img src="/2019/09/24/shap模型解释/image-20190924204404256.png" alt="image-20190924204404256"></p>
<p><img src="/2019/09/24/shap模型解释/image-20190924204419067.png" alt="image-20190924204419067"></p>
<p><img src="/2019/09/24/shap模型解释/image-20190924204433371.png" alt="image-20190924204433371"></p>
<h3 id="5-shap值的解"><a href="#5-shap值的解" class="headerlink" title="5. shap值的解"></a><strong>5. shap值的解</strong></h3><p><img src="/2019/09/24/shap模型解释/image-20190924204605686.png" alt="image-20190924204605686"></p>
<p><img src="/2019/09/24/shap模型解释/image-20190924204621447.png" alt="image-20190924204621447"></p>
<h3 id="6-树模型shap值的解"><a href="#6-树模型shap值的解" class="headerlink" title="6. 树模型shap值的解"></a><strong>6.</strong> <strong>树模型shap值的解</strong></h3><p><img src="/2019/09/24/shap模型解释/image-20190924204654200.png" alt="image-20190924204654200"></p>
<h3 id="7-shap值识别特征交叉"><a href="#7-shap值识别特征交叉" class="headerlink" title="7.shap值识别特征交叉"></a><strong>7.shap值识别特征交叉</strong></h3><p><img src="/2019/09/24/shap模型解释/image-20190924204728110.png" alt="image-20190924204728110"></p>
<h3 id="8-单个特征贡献"><a href="#8-单个特征贡献" class="headerlink" title="8. 单个特征贡献"></a><strong>8.</strong> <strong>单个特征贡献</strong></h3><img src="/2019/09/24/shap模型解释/image-20190924204808921.png" alt="image-20190924204808921" style="zoom: 33%;">

<h3 id="9-xgboost的-Shape-方法使用示例"><a href="#9-xgboost的-Shape-方法使用示例" class="headerlink" title="9.xgboost的 Shape 方法使用示例"></a><strong>9.xgboost的 Shape 方法使用示例</strong></h3><p>XBOOST 0.81以上版本，增加了获取shap值的接口:<br>shap_matrix = xgb_model.predict(X, pred_contribs=True)<br>可以用<strong>shap_matrix</strong>，按照上述的方法计算:样本的shap值和特征的shap值(特征全局重要性)</p>
<h3 id="10-用途"><a href="#10-用途" class="headerlink" title="10 . 用途"></a><strong>10 . 用途</strong></h3><p>一个特征:Gain能选出最重要的特征<br>多个特征:Shap能合理的评估特征的贡献<br>基于Shap值的特征选择优于gain和weight</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">yvelzhang</p>
              <p class="site-description motion-element" itemprop="description">喵喵喵</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yvelzhang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.3</div>


<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style="display:none">
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次
    <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style="display:none">
    有<span id="busuanzi_value_site_uv"></span>人看过我的博客啦
</span>
</div>



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
