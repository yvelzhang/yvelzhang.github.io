<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="喵喵喵">
<meta property="og:type" content="website">
<meta property="og:title" content="yvelzhang">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="yvelzhang">
<meta property="og:description" content="喵喵喵">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="yvelzhang">
<meta name="twitter:description" content="喵喵喵">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>yvelzhang</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">yvelzhang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/23/图学习算法总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/04/23/图学习算法总结/" itemprop="url">图学习算法总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-04-23T17:55:46+08:00">
                2020-04-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/23/多任务学习算法总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/04/23/多任务学习算法总结/" itemprop="url">多任务学习算法总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-04-23T17:55:08+08:00">
                2020-04-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/04/07/多任务学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/04/07/多任务学习/" itemprop="url">多任务学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-04-07T10:05:12+08:00">
                2020-04-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>背景：</strong>只专注于单个模型可能会忽略一些相关任务中可能提升目标任务的潜在信息，通过进行一定程度的共享不同任务之间的参数，可能会使原任务泛化更好。<strong>广义的讲，只要loss有多个就算MTL</strong>，一些别名（joint learning，learning to learn，learning with auxiliary task）</p>
<p><strong>目标：</strong>通过权衡主任务与辅助的相关任务中的训练信息来提升模型的泛化性与表现。从机器学习的视角来看，<strong>MTL可以看作一种inductive transfer</strong>（先验知识），<strong>通过提供inductive bias</strong>（某种对模型的先验假设）<strong>来提升模型效果</strong>。比如，<strong>使用L1正则，我们对模型的假设模型偏向于sparse solution</strong>（参数要少）。<strong>在MTL中，这种先验是通过auxiliary task来提供</strong>，更灵活，告诉模型偏向一些其他任务，最终导致模型会泛化得更好。</p>
<h3 id="MTL-Methods-for-DNN"><a href="#MTL-Methods-for-DNN" class="headerlink" title="MTL Methods for DNN"></a><strong>MTL Methods for DNN</strong></h3><ul>
<li><strong>hard parameter sharing</strong></li>
</ul>
<p>在所有任务中共享一些参数（一般底层），在特定任务层（顶层）使用自己独有参数。这种情况，<strong>共享参数的过拟合几率比较低</strong>（相对非共享参数）</p>
<img src="/2020/04/07/多任务学习/image-20200424192416004.png" alt="image-20200424192416004" style="zoom: 50%;">



<ul>
<li><strong>soft parameter sharing</strong></li>
</ul>
<p>每个任务有自己的参数，最后<strong>通过对不同任务的参数之间的差异加约束</strong>，表达相似性。比如可以使用L2, trace norm等。</p>
<img src="/2020/04/07/多任务学习/image-20200424192507744.png" alt="image-20200424192507744" style="zoom: 50%;">

<p><strong>优点及使用场景</strong></p>
<ol>
<li><strong>implicit data augmentation:</strong> 每个任务多少都有样本噪声，不同的任务可能噪声不同，最终多个任务学习会抵消一部分噪声（类似bagging的思想，不同任务噪声存在于各个方向，最终平均就会趋于零）</li>
<li>一些<strong>噪声很大</strong>的任务，或者<strong>训练样本不足</strong>，<strong>维度高</strong>，模型可能无法有效学习，甚至无法无法学习到相关特征</li>
<li><strong>某些特征可能在主任务不好学习</strong>（比如只存在很高阶的相关性，或被其他因素抑制），<strong>但在辅助任务上好学习</strong>。可以通过辅助任务来学习这些特征，方法比如<strong>hints</strong>（预测重要特征）<strong>[2]</strong> </li>
<li>通过学习足够大的假设空间，<strong>在未来某些新任务中可以有较好的表现（解决冷启动）</strong>，前提是这些任务都是同源的。</li>
<li>作为<strong>一种正则方式，约束模型</strong>。所谓的inductive bias。缓解过拟合，<strong>降低模型的Rademacher complexity</strong>（拟合噪声的能力，用于衡量模型的能力）</li>
</ol>
<h3 id="代表算法"><a href="#代表算法" class="headerlink" title="代表算法"></a>代表算法</h3><h4 id="L2-constrained"><a href="#L2-constrained" class="headerlink" title="L2-constrained"></a>L2-constrained</h4><p>论文：Low Resource Dependency Parsing: Cross-lingual Parameter Sharing in a Neural</p>
<p>多个任务采用相同的结构，关键点在于修改目标函数，加上不同任务相同位置的参数之间的正则化项</p>
<h4 id="Cross-stitch-Networks"><a href="#Cross-stitch-Networks" class="headerlink" title="Cross-stitch Networks"></a>Cross-stitch Networks</h4><p>论文：Cross-Stitch Networks for Multi-Task Learning</p>
<p><strong>soft parameter sharing，</strong>通过<strong>线性组合学习前一层的输出</strong>，允许<strong>模型决定不同任务之间的分享程度</strong></p>
<p>解决问题：针对于不同的多任务学习来说，一般我们需要根据任务需求设计不同的共享层，没有统一的标准。本文就针对这一问题设计了“十字绣”单元，可以通过端对端的学习来自动决定共享层</p>
<img src="/2020/04/07/多任务学习/image-20200424193539446.png" alt="image-20200424193539446" style="zoom: 50%;">

<p>十字绣结构如下，通过在两个网络的特征层之间增加“十字绣”单元可以使网络自动学习到需要共享的特征。其中“十字绣”单元就是一个系数矩阵。其表达式如式（1）所示。从这个公式中可以明显看出，当aAB或aBA值为0时，说明两者没有共享的特征，相反的，当两者的值越大，说明共享部分越大。：</p>
<img src="/2020/04/07/多任务学习/image-20200424193641980.png" alt="image-20200424193641980" style="zoom: 50%;">

<img src="/2020/04/07/多任务学习/image-20200424193917157.png" alt="image-20200424193917157" style="zoom:50%;">

<h4 id="MMOE"><a href="#MMOE" class="headerlink" title="MMOE"></a>MMOE</h4><p>论文：Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts</p>
<p>解决问题：对于hard parameter sharing， 若多个任务的相关性较低，使用相同的共享层参数会影响各个任务的学习效果，本文提出Multi-gate Mixture-of-Experts结构，学习各个任务之间的相关性，每个任务具有不同的experts权重。本文主要借鉴了OMOE（单门控），拓展为多门控结构。</p>
<img src="/2020/04/07/多任务学习/image-20200424194748453.png" alt="image-20200424194748453" style="zoom:50%;">

<p>本文构造了3组不同相关性的任务，并使用三种模型来进行训练，以下为训练效果：</p>
<p><img src="/2020/04/07/多任务学习/image-20200424195224200.png" alt="image-20200424195224200"></p>
<p>可以看出，对于3组任务，MMOE的loss最低，并且受任务相关性的影响最弱。</p>
<h3 id="损失函数权重的设置"><a href="#损失函数权重的设置" class="headerlink" title="损失函数权重的设置"></a>损失函数权重的设置</h3><p><strong>论文：Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</strong></p>
<p><strong>在多任务学习中，如何设置不同损失函数的权重很大程度影响模型的最终结果，本文提出一种基于不确定性系数的方法，让模型自动学习损失函数权重</strong>。</p>
<p><strong>在贝叶斯建模中，需要解决两种不同类型的不确定性。</strong></p>
<ul>
<li><strong>认知不确定性：</strong>表示的是模型本身的不确定性，这是因为缺乏训练数据，模型认知不足，可以通过扩充训练集进行解决；</li>
<li><strong>偶然不确定性：</strong>偶然不确定性表示数据不能解释的信息。</li>
</ul>
<p>偶然不确定性又分成两类：</p>
<ul>
<li><strong>数据依赖(异方差)不确定性：</strong>依赖于输入数据的不确定性，体现在模型的输出上；</li>
<li><strong>任务依赖(同方差)不确定性：</strong>不取决于输入数据，而是取决于不同的任务。</li>
</ul>
<p>在多任务联合学习中，任务依赖不确定性能够表示不同任务间的相对难度。</p>
<p>损失函数的最终形式如下，其中L1、L2为不同任务的损失函数，sigma用来计算损失函数的权重，由模型训练得出：</p>
<img src="/2020/04/07/多任务学习/image-20200427164838393.png" alt="image-20200427164838393" style="zoom: 50%;">

<p>中文讲解：<a href="https://blog.csdn.net/cdknight_happy/article/details/102618883" target="_blank" rel="noopener">https://blog.csdn.net/cdknight_happy/article/details/102618883</a></p>
<p>实现代码例子：<a href="https://github.com/yaringal/multi-task-learning-example" target="_blank" rel="noopener">https://github.com/yaringal/multi-task-learning-example</a></p>
<p><strong>论文：Multi-Task Learning as Multi-Objective Optimization</strong></p>
<p>该文章理论部分较多还没细看，可参考知乎上的讲解：<a href="https://zhuanlan.zhihu.com/p/68846373" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/68846373</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/GBDT原理解读/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/GBDT原理解读/" itemprop="url">GBDT原理解读</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T16:12:01+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考：<a href="https://blog.csdn.net/zpalyq110/article/details/79527653" target="_blank" rel="noopener">https://blog.csdn.net/zpalyq110/article/details/79527653</a></p>
<p>GBDT论文：<a href="http://biostat.jhsph.edu/~mmccall/articles/friedman_1999.pdf" target="_blank" rel="noopener">http://biostat.jhsph.edu/~mmccall/articles/friedman_1999.pdf</a></p>
<p>GBDT使用的决策树是CART回归树（无论是处理回归还是二分类），原因是GBDT每次拟合的都是梯度值（连续值），因此要使用回归树。回归树寻找最佳二分点使用的是平方误差，而不是基尼指数或熵。</p>
<h2 id="回归树生成原理"><a href="#回归树生成原理" class="headerlink" title="回归树生成原理"></a><strong>回归树生成原理</strong></h2><p>在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：</p>
<p>（1）选择最优切分变量j与切分点s，求解</p>
<img src="/2020/03/20/GBDT原理解读/image-20200320164209194.png" alt="image-20200320164209194" style="zoom:50%;">

<p>遍历变量j，对固定的切分变量j扫描切分点s，选择使得上式达到最小值的对(j,s)。其中R1, R2是（j,s）分割出来的两个输入空间，c1、c2分别是这两个空间的输出均值。</p>
<p>（2）用选定的对(j,s)划分区域并决定相应的输出值：</p>
<img src="/2020/03/20/GBDT原理解读/image-20200320164629727.png" alt="image-20200320164629727" style="zoom:50%;">

<p>（3）继续对两个子区域调用步骤（1）和（2），直至满足停止条件</p>
<p>停止条件：</p>
<p>a . 叶子节点的数据y相同</p>
<p>b. 叶子节点样本个数小于阈值</p>
<p>c. 到达深度上限，叶子个数上限</p>
<p>（4）将输入空间划分为M个区域 R1,R2,…RM，生成决策树：</p>
<img src="/2020/03/20/GBDT原理解读/image-20200320164734682.png" alt="image-20200320164734682" style="zoom:50%;">

<p>此方法的复杂度较高，尤其在每次寻找切分点时，需要遍历当前所有特征的所有可能取值，假如总共有F个特征，每个特征有N个取值，生成的决策树有S个内部节点，则该算法的时间复杂度为：O(F x N x S)</p>
<h2 id="Gradient-Boosting：-拟合负梯度"><a href="#Gradient-Boosting：-拟合负梯度" class="headerlink" title="Gradient Boosting： 拟合负梯度"></a>Gradient Boosting： 拟合负梯度</h2><p>梯度提升树（Grandient Boosting）是提升树（Boosting Tree）的一种改进算法，所以在讲梯度提升树之前先来说一下提升树</p>
<p>通俗举例：假如有个人30岁，我们首先用20岁去拟合，发现损失有10岁，这时我们用6岁去拟合剩下的损失，发现差距还有4岁，第三轮我们用3岁拟合剩下的差距，差距就只有一岁了。如果我们的迭代轮数还没有完，可以继续迭代下面，每一轮迭代，拟合的岁数误差都会减小。最后将每次拟合的岁数加起来便是模型输出的结果</p>
<p><img src="/2020/03/20/GBDT原理解读/image-20200321144903747.png" alt="image-20200321144903747"></p>
<p><strong>残差是什么</strong></p>
<img src="/2020/03/20/GBDT原理解读/image-20200321150044162.png" alt="image-20200321150044162" style="zoom:50%;">

<p>当损失函数是平方损失和指数损失函数时，梯度提升树每一步优化是很简单的，但是对于一般损失函数而言，往往每一步优化起来不那么容易，针对这一问题，Friedman提出了梯度提升树算法，这是利用最速下降的近似方法，<strong>其关键是利用损失函数的负梯度作为提升树算法中的残差的近似值。</strong></p>
<p><strong>什么是负梯度？</strong></p>
<img src="/2020/03/20/GBDT原理解读/image-20200321150816821.png" alt="image-20200321150816821" style="zoom:50%;">

<h2 id="GBDT算法流程"><a href="#GBDT算法流程" class="headerlink" title="GBDT算法流程"></a>GBDT算法流程</h2><p><img src="/2020/03/20/GBDT原理解读/image-20200321155314678.png" alt="image-20200321155314678"></p>
<h2 id="GBDT的优缺点"><a href="#GBDT的优缺点" class="headerlink" title="GBDT的优缺点"></a>GBDT的优缺点</h2><p><strong>优点：</strong></p>
<ul>
<li>可以灵活处理各种类型的数据，包括连续值和离散值。</li>
<li>在相对少的调参时间情况下，预测的精度高。这个是相对SVM来说的。</li>
<li>使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>由于弱学习器之间存在依赖关系，难以并行训练数据</li>
<li>如果数据维度较高时会加大算法的计算复杂度</li>
</ul>
<h2 id="其他知识点"><a href="#其他知识点" class="headerlink" title="其他知识点"></a>其他知识点</h2><p><strong>学习率</strong></p>
<p>Shrinkage（缩减）的思想认为，每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。因此每次叠加新的一棵树时，会乘一个学习率，学习率通过线性搜索获得，公式如下：</p>
<img src="/2020/03/20/GBDT原理解读/image-20200321163345096.png" alt="image-20200321163345096" style="zoom:50%;">

<p><strong>提升树和梯度提升树的区别</strong></p>
<ol>
<li>提升树初始化标签为0，GBDT初始化标签为使全局损失函数最小的值。</li>
<li>当损失函数比较简单时（平方损失，指数损失），提升树残差容易计算。对于一般损失函数而言，GBDT使用损失函数的负梯度来近似残差。<strong>残差是能使该样本在本轮损失函数最小的值</strong>（可以回看前面的残差环节）。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/20/lightgbm原理解读/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/20/lightgbm原理解读/" itemprop="url">xgboost/lightgbm原理解读</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-20T16:05:39+08:00">
                2020-03-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>论文：<a href="http://103.95.217.77/papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf" target="_blank" rel="noopener">http://103.95.217.77/papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf</a></p>
<p>源码：<a href="https://github.com/microsoft/LightGBM" target="_blank" rel="noopener">https://github.com/microsoft/LightGBM</a></p>
<p>GBDT原理解读：</p>
<p><a href="http://yvelzhang.site/2020/03/20/GBDT%E5%8E%9F%E7%90%86%E8%A7%A3%E8%AF%BB/" target="_blank" rel="noopener">http://yvelzhang.site/2020/03/20/GBDT%E5%8E%9F%E7%90%86%E8%A7%A3%E8%AF%BB/</a></p>
<h2 id="xgboost优化"><a href="#xgboost优化" class="headerlink" title="xgboost优化"></a>xgboost优化</h2><p>参考文献：<a href="https://www.jianshu.com/p/5e6c5b616114" target="_blank" rel="noopener">https://www.jianshu.com/p/5e6c5b616114</a></p>
<p><strong>xgboost的优化点主要在于：</strong></p>
<ul>
<li><p>XGB利用了二阶梯度来对节点进行划分，相对其他GBM来说，精度更加高</p>
</li>
<li><p>在损失函数中加入了正则项，控制模型的复杂度，提高模型的鲁棒性</p>
</li>
<li><p>利用局部近似算法对分裂节点的贪心算法优化，取适当的eps时，可以保持算法的性能且提高算法的运算速度。</p>
</li>
<li><p>XGB对稀疏的特征划分方式，对缺失值分别尝试左子树和又子树，取损失最低的方向作为默认方向</p>
</li>
<li><p>在寻找最优分割点时进行并行计算，并行的粒度不是树，而是特征。每个特征事先排序好存在一列上，具体做法是对特征取值进行加权分桶，使用样本损失函数的二阶梯度作为权值，在分桶之前对特征进行预排序</p>
</li>
<li><p>引入学习率，其作用是为了给后面的迭代保留优化空间，避免过拟合。</p>
</li>
</ul>
<p><strong>同时xgboost也存在缺点：</strong></p>
<ul>
<li>需要pre-sorted，这个会耗掉很多的内存空间（2 * count(data) * count( features)）</li>
<li>数据分割点上，由于XGB对不同的数据特征使用pre-sorted算法而不同特征其排序顺序是不同的，所以分裂时需要对每个特征单独做依次分割，遍历次数为count(data) * count(features)来将数据分裂到左右子节点上。</li>
<li>尽管使用了局部近似计算，但是处理粒度还是太细了</li>
<li>由于pre-sorted处理数据，在寻找特征分裂点时（level-wise），会产生大量的cache随机访问。</li>
</ul>
<h2 id="lightgbm优化"><a href="#lightgbm优化" class="headerlink" title="lightgbm优化"></a>lightgbm优化</h2><h3 id="1-直方图优化"><a href="#1-直方图优化" class="headerlink" title="1.直方图优化"></a>1.直方图优化</h3><p>参考链接：<a href="https://blog.csdn.net/anshuai_aw1/article/details/83040541" target="_blank" rel="noopener">https://blog.csdn.net/anshuai_aw1/article/details/83040541</a></p>
<p>主要思想：特征值分桶（连续特征，类别特征）</p>
<p>与pre-sort相比</p>
<p>优点：</p>
<ul>
<li>Pre-sorted 算法需要的内存约是训练数据的两倍(2 * #data * #features* 4Bytes)，它需要用32位浮点(4Bytes)来保存 feature value，并且对每一列特征，都需要一个额外的排好序的索引，这也需要32位(4Bytes)的存储空间。因此是(2 * #data * #features* 4Bytes)。而对于 histogram 算法，则只需要(#data * #features * 1Bytes)的内存消耗，仅为 pre-sorted算法的1/8。因为 histogram 算法仅需要存储 feature bin value (离散化后的数值)，不需要原始的 feature value，也不用排序，而 bin value 用 1Bytes(256 bins) 的大小一般也就足够了</li>
<li>计算上的优势则是大幅减少了计算分割点增益的次数。对于每一个特征，pre-sorted 需要对每一个不同特征值都计算一次分割增益，代价是O(#feature<em>#distinct_values_of_the_feature)；而 histogram 只需要计算#bins次，代价是(#feature</em>#bins)</li>
<li>cache-miss，暂时没有细致研究。大概原理是pre-sort对梯度的访问以及索引表的访问存在许多随机访问，容易造成cache miss。</li>
<li>在数据并行的时候，用 histgoram 可以大幅降低通信代价</li>
</ul>
<p>缺点：</p>
<ul>
<li>histogram 算法不能找到很精确的分割点，训练误差没有 pre-sorted 好。但从实验结果来看， histogram 算法在测试集的误差和 pre-sorted 算法差异并不是很大，甚至有时候效果更好。实际上可能决策树对于分割点的精确程度并不太敏感，<strong>而且较“粗”的分割点也自带正则化的效果</strong>，再加上boosting算法本身就是弱分类器的集成</li>
</ul>
<p><strong>直方图做差加速</strong></p>
<p>在histogram算法上一个trick是histogram 做差加速。一个容易观察到的现象：<strong>一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到</strong>。利用这个方法，Lightgbm 可以在构造一个叶子（含有较少数据）的直方图后，可以用非常微小的代价得到它兄弟叶子（含有较多数据）的直方图</p>
<h3 id="2-Gradient-based-One-Side-Sampling（GOSS）"><a href="#2-Gradient-based-One-Side-Sampling（GOSS）" class="headerlink" title="2.  Gradient-based One-Side Sampling（GOSS）"></a>2.  Gradient-based One-Side Sampling（GOSS）</h3><p>gbdt是受欢迎的机器学习算法，当特征维度很高或数据量很大时，有效性和可拓展性没法满足。lightgbm提出GOSS(Gradient-based One-Side Sampling)和EFB(Exclusive Feature Bundling)进行改进。lightgbm与传统的gbdt在达到相同的精确度时，快20倍</p>
<p><strong>具体操作：</strong>对梯度较小的样本进行采样，在计算信息增益时乘比例系数进行放大。</p>
<p><strong>目标：</strong>对数据集进行抽样，减少计算量；使模型更加关注梯度大的样本（信息增益高，更有利于模型训练）；不改变数据的分布。</p>
<p>梯度单边采样，原理：</p>
<p>a. 选取前a%个较大梯度的值作为大梯度值的训练样本</p>
<p>b. 从剩余的1 - a%个较小梯度的值中，我们随机选取其中的b%个作为小梯度值的训练样本</p>
<p>c. 对于较小梯度的样本，也就是b% * len(samples)，我们在梯度(grad,hess)时将其放大(1 - a) / b倍</p>
<img src="/2020/03/20/lightgbm原理解读/image-20200328195338270.png" alt="image-20200328195338270" style="zoom:50%;">

<p><strong>收敛分析表明，GOSS算法不会太多降低训练复杂度，并且超越随机选样本</strong></p>
<h3 id="3-Exclusive-Feature-Bundling-EFB"><a href="#3-Exclusive-Feature-Bundling-EFB" class="headerlink" title="3. Exclusive Feature Bundling(EFB)"></a>3. Exclusive Feature Bundling(EFB)</h3><p>参考:<a href="https://blog.csdn.net/qq_24519677/article/details/82811215" target="_blank" rel="noopener">https://blog.csdn.net/qq_24519677/article/details/82811215</a></p>
<p>Lightgbm实现中不仅进行了数据采样，也进行了特征抽样，使得模型的训练速度进一步的减少。但是该特征抽样又与一般的特征抽样有所不同，是将互斥特征绑定在一起从而减少特征维度。主要思想就是，通常在实际应用中高纬度的数据往往都是稀疏数据（如one-hot编码），这使我们有可能设计一种几乎无损的方法来减少有效特征的数量。尤其，在稀疏特征空间中许多特征都是互斥的（例如，很少同时出现非0值）。这就使我们可以安全的将互斥特征绑定在一起形成一个特征，从而减少特征维度。<strong>但是如何选择要合并的特征？怎样将互斥特征绑定在一起？</strong></p>
<p>对于第一个问题，这是一个NP-hard问题。我们把feature看作是图中的点（V），feature之间的总冲突（不互斥的程度）看作是图中的边（E）。而寻找寻找合并特征且使得合并的bundles个数最小，这是一个<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E5%9B%BE%E7%9D%80%E8%89%B2%E9%97%AE%E9%A2%98" target="_blank" rel="noopener">图着色问题</a>。所以这个找出合并的特征且使得bundles个数最小的问题需要使用近似的贪心算法来完成。</p>
<p><strong>算法介绍</strong>：特征作为顶点，特征间的边权重，用两个特征同时包含非零值的个数表示。权重越小，说明越互斥。对顶点的degrees降序排序，以这个顺序进行搜索。设定最大的权重阈值为K。每个特征顶点与现存的bundle进行顺序访问，如果满足该特征和当前bundle内的顶点边权值和加上bundle本身的边权和，小于阈值，则加入该bundle。如果没有找到满足的bundle，则自己成立一个新bundle。</p>
<p><strong>算法简化</strong>：不用建图，特征中的非零值个数大致可以描述冲突程度，因为非零值个数越多，发生冲突的概率越大。</p>
<img src="/2020/03/20/lightgbm原理解读/image-20200328200036387.png" alt="image-20200328200036387" style="zoom: 33%;">

<p><strong>如何将互斥特征绑定在一起？</strong>LightGBM采用直方图算法，减少寻找最佳分裂点的复杂度。直方图算法，将特征离散到若干个bin中。这里通过将bundle内不同的特征添加一个<strong>偏移量</strong>，使得不同的特征分布到bundle的不同bin内。这里bundle取代了特征，因为观察寻找特征组合的步骤，会有单独的特征形成一个bundle。比如，假设我们有两个特征，特征A的取值范围是[0,10)，而特征B的取值范围是[0,20)，我们可以给特征B增加偏移量10，使得特征B的取值范围为[10, 30)，最后合并特征A和B，形成新的特征，取值范围为[0,30)来取代特征A和特征B。</p>
<img src="/2020/03/20/lightgbm原理解读/image-20200328205103839.png" alt="image-20200328205103839" style="zoom:33%;">

<p>​     当然，Histogram算法并不是完美的。由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；差一点的切分点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在Gradient Boosting的框架下没有太大的影响。</p>
<h3 id="4-Leaf-wise-Best-first-Tree-Growth"><a href="#4-Leaf-wise-Best-first-Tree-Growth" class="headerlink" title="4. Leaf-wise (Best-first) Tree Growth"></a>4. Leaf-wise (Best-first) Tree Growth</h3><p>LightGBM对于树的生长使用的是Leaf-wise，而不是Level-wise。这样的做法主要是因为LightGBM认为Level-wise的做法会产生一些低信息增益的节点，浪费运算资源。<strong>其实通常来说，Level-wise对于防止过拟合还是很有作用的，所以大家都比较青睐与它相比与Leaf-wise</strong>。作者认为Leaf-wise能够追求更好的精度，让产生更好精度的节点做分裂。但这样带来过拟合的问题，所以作者使用的max_depth来控制它的最大高度。<strong>还有原因是因为LightGBM在做数据合并，Histogram Algorithm和GOSS等各个操作，其实都有天然正则化的作用，所以使用Leaf-wise来提高精度是一个很不错的选择。</strong></p>
<img src="/2020/03/20/lightgbm原理解读/image-20200328195842857.png" alt="image-20200328195842857" style="zoom:50%;">

<h3 id="5-类别特征处理"><a href="#5-类别特征处理" class="headerlink" title="5. 类别特征处理"></a>5. 类别特征处理</h3><p>lightgbm采用many vs many的处理方式（除非该特征的类别特别少），具体步骤如下：</p>
<p><strong>Step1：离散特征建立直方图</strong></p>
<p>统计该特征下每一种离散值出现的次数，并从高到低排序，并过滤掉出现次数较少的特征值, 然后为每一个特征值，建立一个bin容器, 对于在bin容器内出现次数较少的特征值直接过滤掉，不建立bin容器</p>
<p><strong>Step2:</strong>  <strong>寻找分割点</strong></p>
<p>a. 先看该特征下划分出的bin容器的个数，如果bin容器的数量小于4，直接使用one vs other方式, 逐个扫描每一个bin容器，找出最佳分裂点。</p>
<p>b. 对于bin容器较多的情况, 先进行过滤，只让子集合较大的bin容器参加划分阈值计算, 对每一个符合条件的bin容器进行公式计算(公式如下: 该bin容器下所有样本的一阶梯度之和 / 该bin容器下所有样本的二阶梯度之和 + 正则项(参数cat_smooth)，这里为什么不是label的均值呢？其实上例中只是为了便于理解，只针对了学习一棵树且是回归问题的情况， 这时候一阶导数是Y, 二阶导数是1)，得到一个值，根据该值对bin容器从小到大进行排序，然后分从左到右、从右到左进行搜索，得到最优分裂阈值。但是有一点，没有搜索所有的bin容器，而是设定了一个搜索bin容器数量的上限值，程序中设定是32，即参数max_num_cat。这32个bin中最优划分的阈值的左边或者右边所有的bin容器就是一个many集合，而其他的bin容器就是另一个many集合。</p>
<p>c.  对于连续特征，划分阈值只有一个，对于离散值可能会有多个划分阈值，每一个划分阈值对应着一个bin容器编号，当使用离散特征进行分裂时，只要数据样本对应的bin容器编号在这些阈值对应的bin集合之中，这条数据就加入分裂后的左子树，否则加入分裂后的右子树</p>
<p><strong>Lightgbm预测时, 对于在训练中没有出现过的类别特征，当成空缺值处理</strong>。</p>
<p><strong>补充：</strong></p>
<p><strong>lightgbm如何处理L1这种二阶倒数(hess)为constant的特征</strong>，在github看到的一段回答：</p>
<img src="/2020/03/20/lightgbm原理解读/image-20200329164741404.png" alt="image-20200329164741404" style="zoom:50%;">

<p>大意是，对于这种特征，在寻找最佳分割点时只使用一阶导数；叶子节点输出使用的是该叶子上所有样本残差的平均（对于正常二阶导数的loss，叶子节点输出为 L’^2/L’’）。</p>
<p><strong>对于用户自己实现的 custom loss，lightgbm默认为non-constant hessian（所以必须保证二阶可导），叶子节点输出L’^2/L’’</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/11/回归问题中的各种损失函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/11/回归问题中的各种损失函数/" itemprop="url">回归问题常用的各种损失函数和评估指标</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-11T14:46:31+08:00">
                2020-03-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>损失函数：L1，L2，huber，log-cosh，quantile，gamma</p>
<p>评估指标：mse，rmse，mae，mape</p>
<h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="L1（又称MAE）"><a href="#L1（又称MAE）" class="headerlink" title="L1（又称MAE）"></a>L1（又称MAE）</h2><img src="/2020/03/11/回归问题中的各种损失函数/image-20200311145739046.png" alt="image-20200311145739046" style="zoom:50%;">

<p>如果对所有样本点只给出一个预测值，那么这个值就是所有目标值的中位数。<br>优点：</p>
<ul>
<li>对异常值具有较好鲁棒性</li>
</ul>
<p>缺点：</p>
<ul>
<li>梯度不变是个严重问题，即使对于很小的损失，梯度也很大，不利于模型收敛（比如神经网络），常使用变化的学习率解决，或者使用huber损失</li>
</ul>
<h2 id="L2（又称MSE）"><a href="#L2（又称MSE）" class="headerlink" title="L2（又称MSE）"></a>L2（又称MSE）</h2><img src="/2020/03/11/回归问题中的各种损失函数/image-20200311145816310.png" alt="image-20200311145816310" style="zoom:50%;">

<p>回归问题中最常见的损失函数。如果对所有样本点只给出一个预测值，那么这个值就是所有目标值的平均值。<br>优点：</p>
<ul>
<li>计算方便，逻辑清晰，衡量误差较准确</li>
<li>梯度随着误差增大或减小，收敛效果好</li>
</ul>
<p>缺点：</p>
<ul>
<li>对异常点(离群值)敏感，异常点会产生较大的损失，模型追求对异常点预估准确从而牺牲整体精度。特别是对于长尾分布的数据（小数值占一大部分，同时会有长尾的大数值），长尾数据产生较大的loss，导致模型过于关注少量长尾数据，从而对于其他数据预估精度降低。</li>
</ul>
<p>例如某用户付费数据分布，大量用户付费在0附近，但是也会有较长的尾部数据，对于这种数据集使用L2，会导致模型倾向于准确预估高付费用户，低付费用户预估精度低，因此推荐使用L1损失：</p>
<img src="/2020/03/11/回归问题中的各种损失函数/image-20200311155538130.png" alt="image-20200311155538130" style="zoom: 33%;">

<p><strong>L1和L2对比：</strong>对异常值而言，中位数比均值更加鲁棒，因此MAE对于异常值也比MSE更稳定</p>
<h2 id="huber"><a href="#huber" class="headerlink" title="huber"></a>huber</h2><img src="/2020/03/11/回归问题中的各种损失函数/image-20200311161547867.png" alt="image-20200311161547867" style="zoom:50%;">

<img src="/2020/03/11/回归问题中的各种损失函数/image-20200311161646504.png" alt="image-20200311161646504" style="zoom: 33%;">

<p>当误差在[0-δ,0+δ]之间时，等价为MSE，而在[-∞,δ]和[δ,+∞]时为MAE<br>优点：</p>
<ul>
<li>对异常值更加鲁棒</li>
<li>在最优点附近由于调整为MSE，梯度更新会随着误差减小而减小，有利于收敛</li>
</ul>
<p>缺点：</p>
<ul>
<li>引入额外的超参，需要调试</li>
<li>临界点delta处不可导</li>
</ul>
<h2 id="log-cosh"><a href="#log-cosh" class="headerlink" title="log-cosh"></a>log-cosh</h2><img src="/2020/03/11/回归问题中的各种损失函数/image-20200311163244042.png" alt="image-20200311163244042" style="zoom:50%;">

<p>比MSE更加平滑的损失函数，对于较小的x，log(cosh(x))近似等于(x^2)/2，对于较大的x，近似等于abs(x)-log(2)。这意味着log-cosh基本类似于均方误差，但不易受到异常点的影响。它具有Huber损失所有的优点，但不同于Huber损失的是，log-cosh二阶处处可微。。<br>优点：</p>
<ul>
<li>具有huber损失具备的所有优点（不受异常点影响，处处可微）</li>
<li>二阶处处可微，许多机器学习算法采用牛顿法逼近最优点，比如鼎鼎大名的XGBoost算法，而牛顿法要求损失函数二阶可微。</li>
</ul>
<p>缺点：</p>
<ul>
<li>误差很大情况下，一阶梯度和Hessian会变成定值，导致XGBoost出现缺少分裂点的情况。</li>
</ul>
<p>二阶可微的好处：凸优化求最小值的充要条件是一阶导数为0，二阶导数＞0。</p>
<h3 id="Quantile-Loss（分位数损失）"><a href="#Quantile-Loss（分位数损失）" class="headerlink" title="Quantile Loss（分位数损失）"></a>Quantile Loss（分位数损失）</h3><p>通常的回归算法是拟合训练数据的期望或者中位数，而使用分位数损失函数可以通过给定不同的分位点，拟合训练数据的不同分位数。分位数回归提出的原因，就是因为不希望仅仅是研究y的期望，而是希望能探索y的<strong>完整分布</strong>状况，或者说可能在某些情况下我们更希望了解y在给定x情况下的某个分位数</p>
<img src="/2020/03/11/回归问题中的各种损失函数/image-20200311165930000.png" alt="image-20200311165930000" style="zoom: 50%;">

<img src="/2020/03/11/回归问题中的各种损失函数/image-20200311170137928.png" alt="image-20200311170137928" style="zoom:50%;">

<p>该函数是一个分段函数，𝛾为分位数系数，𝑦为真实值，𝑓(𝑥)为预测值。根据预测值和真实值的大小，分两种情况来开考虑。𝑦&gt;𝑓(𝑥)为高估，预测值比真实值大；𝑦&lt;𝑓(𝑥)为低估，预测值比真实值小，<strong>使用不同过得系数来控制高估和低估在整个损失值的权重</strong> 。</p>
<p>特别的，当𝛾=0.5时，分位数损失退化为平均绝对误差MAE，也可以将MAE看成是分位数损失的一个特例 - 中位数损失。下图是取不同的中位点[0.25,0.5,0.7]得到不同的分位数损失函数的曲线，也可以看出0.5时就是MAE。</p>
<img src="/2020/03/11/回归问题中的各种损失函数/image-20200311170441005.png" alt="image-20200311170441005" style="zoom: 33%;">

<h1 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h1><h2 id="mse，rmse"><a href="#mse，rmse" class="headerlink" title="mse，rmse"></a>mse，rmse</h2><p>MSE: 等效于L2损失，用MSE来评估算法精度时会对离群值敏感，追求对离群值预估准确从而牺牲整体精度，公式如下：</p>
<img src="/2020/03/11/回归问题中的各种损失函数/image-20200311171821899.png" alt="image-20200311171821899" style="zoom:25%;">

<p>RMSE：MSE开根号，本质上和MSE没区别，只是为了数量级与真实数据相同，更容易理解。</p>
<h2 id="mae"><a href="#mae" class="headerlink" title="mae"></a>mae</h2><img src="/2020/03/11/回归问题中的各种损失函数/image-20200311172144716.png" alt="image-20200311172144716" style="zoom:25%;">

<p>等效于L1损失，用该值评估算法精度能解决MSE对离群值敏感的问题。</p>
<h2 id="mape"><a href="#mape" class="headerlink" title="mape"></a>mape</h2><p>平均绝对百分比误差（Mean Absolute Percentage Error）</p>
<img src="/2020/03/11/回归问题中的各种损失函数/image-20200311172900880.png" alt="image-20200311172900880" style="zoom:50%;">

<h2 id="R2"><a href="#R2" class="headerlink" title="R2"></a>R2</h2><img src="/2020/03/11/回归问题中的各种损失函数/image-20200311184606276.png" alt="image-20200311184606276" style="zoom:33%;">

<p>其中，分子部分表示真实值与预测值的平方差之和，类似于均方差 MSE；分母部分表示真实值与均值的平方差之和，类似于方差 Var。根据 R-Squared 的取值，来判断模型的好坏，其取值范围为[0,1]：</p>
<p>如果结果是 0，说明模型拟合效果很差；如果结果是 1，说明模型无错误。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/17/模型融合blending和stack/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/02/17/模型融合blending和stack/" itemprop="url">集成学习blending和stacking</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-17T16:32:53+08:00">
                2020-02-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>集成学习主要范围如下，本篇主要讲blending和stacking：</p>
<img src="/2020/02/17/模型融合blending和stack/image-20200217163456322.png" alt="image-20200217163456322" style="zoom:50%;">

<h3 id="1-blending"><a href="#1-blending" class="headerlink" title="1.blending"></a>1.blending</h3><p>（1）uniform blending</p>
<p>最终结果是各个子学习器结果的投票（分类问题）或平均（回归问题），各个子学习器的结果具有相同权重。</p>
<p>（2）linear blending</p>
<p><img src="/2020/02/17/模型融合blending和stack/image-20200217164808032.png" alt="image-20200217164808032"></p>
<p>最终结果是各个子学习器（上图中的M1 M2…MM）结果的加权叠加，各个子学习器的结果具有不同权重，通过再训练一个线性模型（上图中的M0）来学习各个子模型的权重（子模型的输出即为第二层线性模型的输入特征）</p>
<p><strong>将原始的训练数据集按照一定的比例分为训练数据集DT和验证(或者说测试)数据集DA，算法训练过程如下：</strong></p>
<p>假设要构建M个模型M1，M2，……MM。以模型Mi为例说明：对训练数据集DT进行学习，得到模型Mi。学习完毕后对验证数据DA的计算结果为DA_Mi，对预测数据集合的计算结果为DP_Mi。待M个模型全部建立完成后：</p>
<ol>
<li>对验证数据得到的结果集合为DA_M1，DA_M2，……，DA_MM，其中每个结果序列都可看作一个新的特征，将这些新的特征排列起来看作一个训练数据集的输入，输出就是原始数据集中验证数据集的输出。输出和输入结合起来构成一个完整的训练数据集；</li>
<li>对预测数据得到的结果集合为DP_M1，DP_M2，……，DP_MM，其中每个结果序列都可看作一个新的特征，将这些新的特征排列起来看作一个预测数据集的输入；</li>
<li>对a中得到的训练数据集建立模型M0，学习完毕后，该模型对于b中得到的预测数据集进行计算，得到最终的结果；</li>
</ol>
<p>（3）any blending</p>
<p>与linear blending相似，唯一不同是子模型的权重可以由任何模型（不一定要是线性模型）来进行训练。</p>
<h3 id="2-stacking"><a href="#2-stacking" class="headerlink" title="2.stacking"></a>2.stacking</h3><p>代码参考（<a href="https://blog.csdn.net/zgj_gutou/article/details/88598934）" target="_blank" rel="noopener">https://blog.csdn.net/zgj_gutou/article/details/88598934）</a></p>
<p>结构示意如下：</p>
<img src="/2020/02/17/模型融合blending和stack/image-20200217195553120.png" alt="image-20200217195553120" style="zoom: 50%;">

<p>这里我们用二分类的例子做介绍。<br>例如我们用 RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier 作为第一层学习器（当然这里我们可以添加更多的分类器，也可以用不同的特征组合但是同样的学习方法作为基分类器）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">clfs = [</span><br><span class="line">        RandomForestClassifier(n_estimators = n_trees, criterion = <span class="string">'gini'</span>),</span><br><span class="line">        ExtraTreesClassifier(n_estimators = n_trees * 2, criterion = <span class="string">'gini'</span>),</span><br><span class="line">        GradientBoostingClassifier(n_estimators = n_trees),</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure>

<p>step1:** 训练第一层学习器，并得到第二层学习器所需要的数据，这里会用到 k 折交叉验证。我们首先会将数据集进行一个划分，比如使用80%的训练数据来训练，20%的数据用来测试</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dev_cutoff = len(Y) * <span class="number">4</span>/<span class="number">5</span></span><br><span class="line">    X_dev = X[<span class="symbol">:dev_cutoff</span>]</span><br><span class="line">    Y_dev = Y[<span class="symbol">:dev_cutoff</span>]</span><br><span class="line">    X_test = X[<span class="symbol">dev_cutoff:</span>]</span><br><span class="line">    Y_test = Y[<span class="symbol">dev_cutoff:</span>]</span><br></pre></td></tr></table></figure>

<p>然后对训练数据通过交叉验证训练 clf，并得到第二层的训练数据 blend_train,同时，在每个基分类器的每一折交叉验证中，我们都会对测试数据进行一次预测，以得到我们blend_test，二者的定义如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blend_train = np.zeros((X_dev.shape[0], len(clfs))) # Number of training data x Number of classifiers</span><br><span class="line">blend_test = np.zeros((X_test.shape[0], len(clfs))) # Number of testing data x Number of classifiers</span><br></pre></td></tr></table></figure>

<p>按照上面说的，blend_train基于下面的方法得到，注意，下图是对于一个分类器来说的，所以每个分类器得到的blend_train的行数与用于训练的数据一样多，所以blend_train的shape为X_dev.shape[0]*len(clfs)，即训练集长度 * 基分类器个数：</p>
<p><img src="/2020/02/17/模型融合blending和stack/image-20200217200632015.png" alt="image-20200217200632015"></p>
<p>而对于第二轮的测试集blend_test来说，由于每次交叉验证的过程中都要进行一次预测，假设我们是5折交叉验证，那么对于每个分类器来说，得到的blend_test的shape是测试集行数 * 交叉验证折数，此时的做法是，对axis=1方向取平均值，以得到测试集行数 * 1 的测试数据，所以总的blend_test就是测试集行数 * 基分类器个数，可以跟blend_train保持一致：</p>
<img src="/2020/02/17/模型融合blending和stack/image-20200217200656391.png" alt="image-20200217200656391" style="zoom:50%;">

<p>得到blend_train 和 blend_test的代码如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> enumerate(clfs):</span><br><span class="line">        <span class="built_in">print</span> <span class="string">'Training classifier [%s]'</span> % (j)</span><br><span class="line">        blend_test_j = np.zeros((X_test.shape[0], len(skf))) <span class="comment"># Number of testing data x Number of folds , we will take the mean of the predictions later</span></span><br><span class="line">        <span class="keyword">for</span> i, (train_index, cv_index) <span class="keyword">in</span> enumerate(skf):</span><br><span class="line">            <span class="built_in">print</span> <span class="string">'Fold [%s]'</span> % (i)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># This is the training and validation set</span></span><br><span class="line">            X_train = X_dev[train_index]</span><br><span class="line">            Y_train = Y_dev[train_index]</span><br><span class="line">            X_cv = X_dev[cv_index]</span><br><span class="line">            Y_cv = Y_dev[cv_index]</span><br><span class="line">            </span><br><span class="line">            clf.fit(X_train, Y_train)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># This output will be the basis for our blended classifier to train against,</span></span><br><span class="line">            <span class="comment"># which is also the output of our classifiers</span></span><br><span class="line">            blend_train[cv_index, j] = clf.predict(X_cv)</span><br><span class="line">            blend_test_j[:, i] = clf.predict(X_test)</span><br><span class="line">        <span class="comment"># Take the mean of the predictions of the cross validation set</span></span><br><span class="line">        blend_test[:, j] = blend_test_j.mean(1)</span><br></pre></td></tr></table></figure>

<p>接着我们就可以用 blend_train, Y_dev 去训练第二层的学习器 LogisticRegression(当然也可以是别的分类器，比如lightGBM，XGBoost）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bclf = LogisticRegression()</span><br><span class="line">bclf.fit(blend_train, Y_dev)</span><br></pre></td></tr></table></figure>

<p>最后，基于我们训练的二级分类器，我们可以预测测试集 blend_test，并得到 score：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Y_test_predict = bclf.predict(blend_test)</span><br><span class="line">score = metrics.accuracy_score(Y_test, Y_test_predict)</span><br><span class="line"><span class="built_in">print</span> <span class="string">'Accuracy = %s'</span> % (score)</span><br></pre></td></tr></table></figure>

<p>如果是多分类怎么办呢，我们这里就不能用predict方法，我么要用的是predict_proba方法，得到基分类器对每个类的预测概率代入二级分类器中训练，修改的部分代码如下：</p>
<figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">blend_train = np.zeros((np.<span class="keyword">array</span>(X_dev.values.tolist()).shape[<span class="number">0</span>], num_classes*len(clfs)),dtype=np.float32)  <span class="comment"># Number of training data x Number of classifiers</span></span><br><span class="line">blend_test = np.zeros((np.<span class="keyword">array</span>(X_test.values.tolist()).shape[<span class="number">0</span>], num_classes*len(clfs)),dtype=np.float32)  <span class="comment"># Number of testing data x Number of classifiers</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># For each classifier, we train the number of fold times (=len(skf))</span></span><br><span class="line">    <span class="keyword">for</span> j, clf in enumerate(clfs):</span><br><span class="line">        <span class="keyword">for</span> i, (train_index, cv_index) in enumerate(skf):</span><br><span class="line">            <span class="keyword">print</span>(<span class="string">'Fold [%s]'</span> % (i))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># This is the training and validation set</span></span><br><span class="line">            X_train = X_dev[train_index]</span><br><span class="line">            Y_train = Y_dev[train_index]</span><br><span class="line">            X_cv = X_dev[cv_index]</span><br><span class="line"></span><br><span class="line">            X_train = np.concatenate((X_train, ret_x),axis=<span class="number">0</span>)</span><br><span class="line">            Y_train = np.concatenate((Y_train, ret_y),axis=<span class="number">0</span>)</span><br><span class="line">            clf.fit(X_train, Y_train)</span><br><span class="line">            blend_train[cv_index, j*num_classes:(j+<span class="number">1</span>)*num_classes] = clf.predict_proba(X_cv)</span><br><span class="line">            blend_test[:, j*num_classes:(j+<span class="number">1</span>)*num_classes] += clf.predict_proba(X_test)</span><br><span class="line">blend_test = blend_test / float(n_folds)</span><br></pre></td></tr></table></figure>

<p>上面的代码修改的主要就是blend_train和blend_test的shape，可以看到，对于多分类问题来说，二者的第二维的shape不再是基分类器的数量，而是class的数量*基分类器的数量，这是大家要注意的，否则可能不会得到我们想要的结果。</p>
<h3 id="3-Stacking-和-blending的区别"><a href="#3-Stacking-和-blending的区别" class="headerlink" title="3.Stacking 和 blending的区别"></a>3.Stacking 和 blending的区别</h3><p>bending和stacking的区别：Blending的主要区别在于训练集不是通过K-Fold的CV策略来获得预测值从而生成第二阶段模型的特征，而是建立一个Holdout集，例如10%的训练数据，第二阶段的stacker模型就基于第一阶段模型对这10%训练数据的预测值进行拟合</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/11/01/spark知识点/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/11/01/spark知识点/" itemprop="url">spark知识点</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-11-01T20:12:42+08:00">
                2019-11-01
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="sparkCore（RDD）"><a href="#sparkCore（RDD）" class="headerlink" title="sparkCore（RDD）"></a>sparkCore（RDD）</h3><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a><strong>特点</strong></h4><p>分区：</p>
<p>数据是由多个分区组成</p>
<p>可以指定分区的方式</p>
<p>抽象：</p>
<p>每个分区不一定有物理存储</p>
<p>只能通过接口获取分区数据（只读：只能RDD-&gt;RDD）</p>
<h4 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h4><p>两类算子：transformations 和action<br>窄依赖：分区一对一<br>宽依赖：分区多对多，需shuffle</p>
<p><strong>Transformation算子：</strong></p>
<p>map/filter/flatmap/mapPartitions/mapPartitionsWithIndex/sample/union/intersection</p>
<p>distinct/groupByKey/reduceByKey/aggregateByKey/sortByKey/join/cogroup/cartesian/pipe</p>
<p>coalesce/repatition/repartitionAndSortWithinPartitions</p>
<p><strong>Action算子</strong></p>
<p>reduce/collect/count/first/takeSample/take/takeOrdered/saveAsTextFile/repartitionAndSortWithinPartitions</p>
<p>saveAsObjectFile/countByKey/foreach</p>
<p>详细应用参照链接 <a href="https://blog.csdn.net/Fortuna_i/article/details/81170565" target="_blank" rel="noopener">https://blog.csdn.net/Fortuna_i/article/details/81170565</a></p>
<h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><p>job：action为界<br>stage：job的子集，以shuffle为界（宽依赖）<br>task：stage的子集，根据分区数量为定</p>
<p>Stage调度执行：从最后一个stage反向回溯，寻找所依赖的stage</p>
<p>Task调度：开启一个线程循环，不断根据当前可用资源去取task执行</p>
<h4 id="资源配置"><a href="#资源配置" class="headerlink" title="资源配置"></a>资源配置</h4><p>stage的分区数量决定task有多少</p>
<p>单个executor有多个core，每个core运行一个task，单个executor多个task共享内存</p>
<p>建议避免：<br>太多的cpu或者太多的executor<br>太多分区（任务过细，轮数过多，单个core不超过10个partition）</p>
<h4 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h4><p>过程分为二阶段 write &amp; read<br>引发shuffle操作的算子：repartition/*bykey/join &amp; cogroup</p>
<p>代价很高（序列化，跨机器，读写文件）</p>
<p><strong>阶段一 write：</strong><br>datafile： 使用map分区（hash），每个分区内数据按照记录排序<br>index file： 索引文件</p>
<p><strong>阶段二 read：</strong><br>每个分区，根据索引文件，从相应datafile中读取数据，做归并排序</p>
<p><strong>shuffle调优</strong></p>
<p>a.优化数据结构：</p>
<p>尽可能使用原生类型（Int，Long，Double）</p>
<p>尽可能使用对象数组以及原生类型数组以替代Java或者scala集合类</p>
<p>尽可能避免采用潜逃数据结构保存小对象</p>
<p>b.主动shuffle repartition：</p>
<p>如果分区较少, 可加大分区，将任务细分</p>
<p>c.提高后续分布式运行的速度</p>
<p>调整shuffle read 并发（根据内存大小）：maxSizeInFlight参数</p>
<p>d.避免使用groupbykey （OOM风险，单个key连续内存） 可以用reducebykey/aggregateByKey优化</p>
<p>例子：groupByKey.mapValues(_.sum) </p>
<p>可以用rdd.reduceByKey(_ _+ _)来代替</p>
<h4 id="broadcast"><a href="#broadcast" class="headerlink" title="broadcast"></a>broadcast</h4><p>广播大对象（超过10M）：每个executor传输一次大对象，任务用到的时候直接本地取用，不需要每次都从driver传输</p>
<p>broadcast+map 替代 大表join小表：将小表broadcast，避免shuffle</p>
<p>解决数据倾斜join：将rdd分割成两部分进行join（大量数据的key单独分出来，进行broadcast+map，其他数据照常，最后union）</p>
<h4 id="cache"><a href="#cache" class="headerlink" title="cache"></a>cache</h4><p>persist原则 树形rdd分叉处persist<br>内存充足时 MEMORY_ONLY<br>稀缺时 MEMORY_ONLY_SER<br>谨慎使用 DISK_ONLY</p>
<p>unpersist原则 不使用时尽快unpersist</p>
<h4 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a>checkpoint</h4><p>截断rdd，并且保存到内存，提高链式rdd(依赖关系特别长)的可靠性</p>
<h3 id="sparkSQL"><a href="#sparkSQL" class="headerlink" title="sparkSQL"></a>sparkSQL</h3><p><strong>特点</strong></p>
<p>引入更高级的API（SQLtext  dataframe/dataset）<br>使用高级API时系统会自动优化，例如broadcast</p>
<p><strong>join</strong></p>
<p>inner join 大表小表前后顺序无所谓，底层会自动优化（大表为流失遍历表，小表为查找表）<br>left join 左表为流失遍历表，右表为查找表</p>
<p>关于Dataset常用API，以后再记录。</p>
<p><strong>官方API文档</strong></p>
<p><a href="http://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">http://spark.apache.org/docs/2.2.1/api/java/org/apache/spark/sql/Dataset.html</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/21/ctr预估常用深度模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/21/ctr预估常用深度模型/" itemprop="url">ctr预估常用深度模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-21T08:55:00+08:00">
                2019-10-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>deepFM</p>
<p>NFM</p>
<p>AFM</p>
<p>wide&amp;deep</p>
<p>FNN</p>
<p>PNN</p>
<p>Deep cross</p>
<p>xdeepFM</p>
<p>Din</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/20/ctr预估常用模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/20/ctr预估常用模型/" itemprop="url">ctr预估常用现性模型（LR， FM， FFM）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-20T22:06:07+08:00">
                2019-10-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="LR"><a href="#LR" class="headerlink" title="LR"></a>LR</h2><p><strong>优点：</strong></p>
<p>实现容易，模型可解释</p>
<p><strong>缺点：</strong></p>
<p>只能学习线性关系</p>
<p>需要手动进行特征交叉（二阶，三阶交叉），工作量大，根据业务知识</p>
<p>特征工程工作量大，连续特征需要离散化</p>
<h2 id="FM-（2010年）"><a href="#FM-（2010年）" class="headerlink" title="FM （2010年）"></a>FM （2010年）</h2><p><strong>原理</strong>：</p>
<p>onehot的编码后特征极度稀疏，特征空间大。</p>
<p>通过多项式交叉后，交叉特征的非零样本更加稀疏；采用一种矩阵分解的思路，为每个单特征学习一个k维向量v（只要该非零特征出现了，v便能得到训练）。</p>
<p>复杂度线性/样本稀疏的情况下有优势。</p>
<p><strong>该模型公式如下：</strong></p>
<p><img src="/2019/10/20/ctr预估常用模型/585228-20171123143833790-1275951688.png" alt="img"></p>
<h2 id="FMM-（2015年）"><a href="#FMM-（2015年）" class="headerlink" title="FMM （2015年）"></a>FMM （2015年）</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">yvelzhang</p>
              <p class="site-description motion-element" itemprop="description">喵喵喵</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yvelzhang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.3</div>


<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style="display:none">
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次
    <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style="display:none">
    有<span id="busuanzi_value_site_uv"></span>人看过我的博客啦
</span>
</div>



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
