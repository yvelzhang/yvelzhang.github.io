<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="tensorflow,">










<meta name="description" content="importing data　　dataset API使您能够从简单、可重用的数据切片中构建复杂的输入管道。例如,一个图像模型的管道可能从分布式文件系统中聚合数据,对每个图像应用随机扰动,并将随机选择图像合并为一个batch进行训练。文本模型的管道可能涉及从原料中提取符号文本数据,将它们转换为嵌入标识符查找表,和不同长度的序列。Dataset API很容易处理大量的数据, 不同的数据格式和复杂的转">
<meta name="keywords" content="tensorflow">
<meta property="og:type" content="article">
<meta property="og:title" content="Dataset API详解">
<meta property="og:url" content="http://yoursite.com/2017/11/03/Dataset API/index.html">
<meta property="og:site_name" content="yvelzhang">
<meta property="og:description" content="importing data　　dataset API使您能够从简单、可重用的数据切片中构建复杂的输入管道。例如,一个图像模型的管道可能从分布式文件系统中聚合数据,对每个图像应用随机扰动,并将随机选择图像合并为一个batch进行训练。文本模型的管道可能涉及从原料中提取符号文本数据,将它们转换为嵌入标识符查找表,和不同长度的序列。Dataset API很容易处理大量的数据, 不同的数据格式和复杂的转">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2018-06-14T00:45:02.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dataset API详解">
<meta name="twitter:description" content="importing data　　dataset API使您能够从简单、可重用的数据切片中构建复杂的输入管道。例如,一个图像模型的管道可能从分布式文件系统中聚合数据,对每个图像应用随机扰动,并将随机选择图像合并为一个batch进行训练。文本模型的管道可能涉及从原料中提取符号文本数据,将它们转换为嵌入标识符查找表,和不同长度的序列。Dataset API很容易处理大量的数据, 不同的数据格式和复杂的转">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/11/03/Dataset API/">





  <title>Dataset API详解 | yvelzhang</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">yvelzhang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/11/03/Dataset API/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yvelzhang">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yvelzhang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Dataset API详解</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-03T14:52:31+08:00">
                2017-11-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <span class="post-meta-divider">|</span>
            <span id="busuanzi_value_page_pv"></span>次阅读
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="importing-data"><a href="#importing-data" class="headerlink" title="importing data"></a>importing data</h1><p>　　dataset API使您能够从简单、可重用的数据切片中构建复杂的输入管道。例如,一个图像模型的管道可能从分布式文件系统中聚合数据,对每个图像应用随机扰动,并将随机选择图像合并为一个batch进行训练。文本模型的管道可能涉及从原料中提取符号文本数据,将它们转换为嵌入标识符查找表,和不同长度的序列。Dataset API很容易处理大量的数据, 不同的数据格式和复杂的转换。<br>　　Dataset API 向tensorflow中引入了两个新的抽象：<br>　　tf.contrib.data.Dataset 代表一个元素的序列,其中每个元素包含一个或多个张量对象。例如,在一个图像管道中,一个元素可能是一个训练的例子,用一对张量表示图像数据和标签。有两种截然不同的方法来创建一个数据集:</p>
<ol>
<li><p>从多个tensor对象中构造 (e.g. Dataset.from_tensor_slices()) </p>
</li>
<li><p>从多个tf.contrib.data.Dataset中转化一个dataset (e.g. Dataset.batch())  </p>
</li>
</ol>
<p>　　tf.contrib.data.Iterator 提供了从dataset中提取元素的主要方法。执行此操作时 Iterator.get_next()返回数据集中的下一个元素，通常作为管道与模型之间的接口。最简单的iterator是”one-shot iterator”，它指向一个特定的数据集并且只遍历一次。对于更复杂的应用，Iterator.initializer 操作</p>
<h1 id="基础教学"><a href="#基础教学" class="headerlink" title="基础教学"></a>基础教学</h1><p>　　这部分的指南描述了创建不同类型的Dataset和iterator对象,以及如何提取数据。<br>　　要开始一个输入管道，首先要定义一个source。例如，要从内存中的tensors中构建一个Dataset，可以用 tf.contrib.data.Dataset. from_tensors()以及f.contrib.data.Dataset. from_tensor_slices()。此外，如果输入数据是磁盘中的TFRecord格式，可以构建tf.contrib. data.TFRecordDataset。<br>　　一旦我们有了Dataset对象，我们可以将它转化为新的Dataset通过tf.contrib.data.Dataset对象。例如，你可以对每个元素应用转换正如Dataset.map()，或者多元素的转换正如Dataset.batch()全部的操作种类请见 <a href="https://www.tensorflow.org/api\_docs/python/tf/contrib/data/Dataset" target="_blank" rel="noopener">https://www.tensorflow.org/api\_docs/python/tf/contrib/data/Dataset</a><br>　　最常用的访问Dataset中元素的方式是使用迭代器，每次只访问一个元素（Dataset.make_one_shot_iterator()），tf.contrib.data.Iterator提供两个操作数，Iterator.initializer用来初始化迭代器的状态；Iterator.get_ next() 返回下一个（批）元素所对应的tf.Tensor。根据你的情况,你可以选择不同类型的迭代器,下面列出了选项。</p>
<h1 id="Dataset的结构"><a href="#Dataset的结构" class="headerlink" title="Dataset的结构"></a>Dataset的结构</h1><p>　　一个Dataset由多个结构相同的elements组成，每个element有一个或多个tf.tensors对象被称作components。每个component有自己的tf.DType和tf.TensorShape。Dataset.output_types和Dataset.output_shapes被用来观察每个dataset element中每个component的属性和形状，这些嵌套结构的属性映射到结构中的某一个元素,这可能是一个tensor,tensor的元组或嵌套的tensor的元组。例如：</p>
<pre><code>dataset1 = tf.contrib.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))
print(dataset1.output_types)  # ==&gt; &quot;tf.float32&quot;
print(dataset1.output_shapes)  # ==&gt; &quot;(10,)&quot;

dataset2 = tf.contrib.data.Dataset.from_tensor_slices(
   (tf.random_uniform([4]),
tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))
print(dataset2.output_types)  # ==&gt; &quot;(tf.float32, tf.int32)&quot;
print(dataset2.output_shapes)  # ==&gt; &quot;((), (100,))&quot;

dataset3 = tf.contrib.data.Dataset.zip((dataset1, dataset2))
print(dataset3.output_types)  # ==&gt; (tf.float32, (tf.float32, tf.int32))
print(dataset3.output_shapes)  # ==&gt; &quot;(10, ((), (100,)))&quot;</code></pre><p>　　对element中的每个component进行命名是非常方便的。如果它们表示一个训练例子中的不同特征。除了元组，可以使用collections.namedtuple或者字典映射到tensors来表示dataset中的单个element。    </p>
<pre><code>dataset = tf.contrib.data.Dataset.from_tensor_slices(
   {&quot;a&quot;: tf.random_uniform([4]),&quot;b&quot;: tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})  
print(dataset.output_types)  # ==&gt; &quot;{&apos;a&apos;: tf.float32, &apos;b&apos;: tf.int32}&quot;
print(dataset.output_shapes)  # ==&gt; &quot;{&apos;a&apos;: (), &apos;b&apos;: (100,)}&quot;
　　　　</code></pre><p>　　Dataset之间的转化适合任何结构，Dataset.map()，Dataset.flat_ map()和Dataset.filter()转化，表示逐元素地进行映射，元素的结构决定了函数的参数：</p>
<pre><code>dataset1 = dataset1.map(lambda x: ...)

dataset2 = dataset2.flat_map(lambda x, y: ...)

dataset3 = dataset3.filter(lambda x, (y, z): ...)</code></pre><h1 id="创建迭代器"><a href="#创建迭代器" class="headerlink" title="创建迭代器"></a>创建迭代器</h1><p>　　当为输入数据建立了Dataset后，下一步就是创建迭代器去访问Dataset中的元素，Dataset中的API目前支持三种迭代器：   </p>
<ul>
<li><p>one-shot,</p>
</li>
<li><p>initializable,</p>
</li>
<li><p>reinitializable, and</p>
</li>
<li><p>feedable.</p>
</li>
</ul>
<p>　　one-shot迭代器是最简单的迭代器，它只支持遍历一次dataset，且不需要显示的初始化。one-shot迭代器能处理现有基于队列的输入管道所支持的所有情况，但是它不支持参数，例子如下：  </p>
<pre><code>dataset = tf.contrib.data.Dataset.range(100)
iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

for i in range(100):
  value = sess.run(next_element)
  assert i == value  </code></pre><p>　　initializable迭代器在使用前需要一个显式的iterator.initializer操作，它使您可以参数化数据集的定义，使用一个或多个tf.placeholder() tensors，当对迭代器进行initialize的时候对这些tensors进行fed操作。例子如下：  </p>
<pre><code>max_value = tf.placeholder(tf.int64, shape=[])
dataset = tf.contrib.data.Dataset.range(max_value)
iterator = dataset.make_initializable_iterator()
next_element = iterator.get_next()

# Initialize an iterator over a dataset with 10 elements.
sess.run(iterator.initializer, feed_dict={max_value: 10})
for i in range(10):
  value = sess.run(next_element)
  assert i == value

# Initialize the same iterator over a dataset with 100 elements.
sess.run(iterator.initializer, feed_dict={max_value: 100})
for i in range(100):
  value = sess.run(next_element)
  assert i == value  </code></pre><p>　　reinitializable迭代器能从多个不同的Dataset对象中进行初始化，例如,你可能有一个训练输入管道,使用随机扰动输入图像提高泛化,以及一个验证输入管道在未修改的数据集上进行评估。这些管道通常会使用不同的Dataset对象，但是这些对象有相同的结构(即每个component都有相同的类型和兼容的形状)。  </p>
<pre><code> #Define training and validation datasets with the same structure.
training_dataset = tf.contrib.data.Dataset.range(100).map(
lambda x: x + tf.random_uniform([], -10, 10, tf.int64))
validation_dataset = tf.contrib.data.Dataset.range(50)

 #A reinitializable iterator is defined by its structure. We could use the
 # `output_types` and `output_shapes` properties of either `training_dataset`
 # or `validation_dataset` here, because they are compatible.
iterator = Iterator.from_structure(training_dataset.output_types,
   training_dataset.output_shapes)
next_element = iterator.get_next()

training_init_op = iterator.make_initializer(training_dataset)
validation_init_op = iterator.make_initializer(validation_dataset)

 # Run 20 epochs in which the training dataset is traversed, followed by the
 # validation dataset.
for _ in range(20):
  # Initialize an iterator over the training dataset.
  sess.run(training_init_op)
  for _ in range(100):
sess.run(next_element)

  # Initialize an iterator over the validation dataset.
  sess.run(validation_init_op)
  for _ in range(50):
sess.run(next_element)  </code></pre><p>　　feedable迭代器能与tf.placeholder一起使用，来选择每次调用tf.Session.run所使用的迭代器，通过熟悉的feed_dict机制。它提供了reinitializable迭代器相同的功能，但它不需要在一开始初始迭代器，和上面一样的例子作为比较，可以使用tf.contrib.data.Iterator.from_string_handle 来定义一个feedable迭代器实现在两个数据集间进行切换。</p>
<pre><code>training_dataset = tf.contrib.data.Dataset.range(100).map(
lambda x: x + tf.random_uniform([], -10, 10, tf.int64)).repeat()
validation_dataset = tf.contrib.data.Dataset.range(50)

　# A feedable iterator is defined by a handle placeholder and its structure. We
　# could use the `output_types` and `output_shapes` properties of either
　# `training_dataset` or `validation_dataset` here, because they have
　# identical structure.
handle = tf.placeholder(tf.string, shape=[])
iterator = tf.contrib.data.Iterator.from_string_handle(
handle, training_dataset.output_types, training_dataset.output_shapes)
next_element = iterator.get_next()

　# You can use feedable iterators with a variety of different kinds of iterator
　# (such as one-shot and initializable iterators).
training_iterator = training_dataset.make_one_shot_iterator()
validation_iterator = validation_dataset.make_initializable_iterator()

　# The `Iterator.string_handle()` method returns a tensor that can be evaluated
　# and used to feed the `handle` placeholder.
training_handle = sess.run(training_iterator.string_handle())
validation_handle = sess.run(validation_iterator.string_handle())

　# Loop forever, alternating between training and validation.
while True:
  # Run 200 steps using the training dataset. Note that the training dataset is
  # infinite, and we resume from where we left off in the previous `while` loop
  # iteration.
  for _ in range(200):
sess.run(next_element, feed_dict={handle: training_handle})

  # Run one pass over the validation dataset.
  sess.run(validation_iterator.initializer)
  for _ in range(50):
sess.run(next_element, feed_dict={handle: validation_handle})
　　</code></pre><h1 id="从迭代器中获得值"><a href="#从迭代器中获得值" class="headerlink" title="从迭代器中获得值"></a>从迭代器中获得值</h1><p>　　使用Iterator.get_next()，每使用一次，迭代器取得下一个元素的值（并不是调用Iterator.get_next()迭代器就会前进，必须要使用所返回的tensor，通过tf.Session.run()来获得下一个值，迭代器才会前进）。<br>　　如果迭代器到达的数据集的末尾，Iterator.get_next()将会返回tf.errors.OutOfRangeError，在这之后迭代器将会进入不安全状态，必须要重新进行初始化才能继续使用。</p>
<pre><code>dataset = tf.contrib.data.Dataset.range(5)
iterator = dataset.make_initializable_iterator()
next_element = iterator.get_next()

　# Typically `result` will be the output of a model, or an optimizer&apos;s
　# training operation.
result = tf.add(next_element, next_element)

sess.run(iterator.initializer)
print(sess.run(result))  # ==&gt; &quot;0&quot;
print(sess.run(result))  # ==&gt; &quot;2&quot;
print(sess.run(result))  # ==&gt; &quot;4&quot;
print(sess.run(result))  # ==&gt; &quot;6&quot;
print(sess.run(result))  # ==&gt; &quot;8&quot;
try:
  sess.run(result)
except tf.errors.OutOfRangeError:
  print(&quot;End of dataset&quot;)  # ==&gt; &quot;End of dataset&quot;　</code></pre><p>　　通用模式为：  </p>
<pre><code>sess.run(iterator.initializer)
while True:
  try:
sess.run(result)
  except tf.errors.OutOfRangeError:
break  </code></pre><p>　　如果dataset中的每个元素都有嵌套结构，Iterator.get_next()返回的tensors也具有相同的嵌套结构。  </p>
<pre><code>dataset1 = tf.contrib.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))
dataset2 = tf.contrib.data.Dataset.from_tensor_slices((tf.random_uniform([4]), tf.random_uniform([4, 100])))
dataset3 = tf.contrib.data.Dataset.zip((dataset1, dataset2))

iterator = dataset3.make_initializable_iterator()

sess.run(iterator.initializer)
next1, (next2, next3) = iterator.get_next()  </code></pre><h1 id="读取输入值"><a href="#读取输入值" class="headerlink" title="读取输入值"></a>读取输入值</h1><h3 id="读numpy数组"><a href="#读numpy数组" class="headerlink" title="读numpy数组"></a>读numpy数组</h3><p>　　如果数据都保存在内存中，最简单的办法就是从数据直接创建Dataset然后将它们转换为tensor对象，使用Dataset.from_tensor_slices()。 </p>
<pre><code>with np.load(&quot;/var/data/training_data.npy&quot;) as data:  
features = data[&quot;features&quot;]
labels = data[&quot;labels&quot;]
assert features.shape[0] == labels.shape[0]
dataset = tf.data.Dataset.from_tensor_slices((features, labels))</code></pre><p>　　要注意的是，以上代码将会把features和labels数组嵌入到graph中，类似tf.constant()操作，这种操作对于小数据还好，但是很浪费内存，因为数组中的内容会被复制好多次，可能会触及tf.GraphDef protocol buffer中2GB的上限。<br>　　可以使用tf.placeholder()tensors来定义Dataset，然后在初始化迭代器的时候对数组进行feed。  </p>
<pre><code> # Load the training data into two NumPy arrays, for example using `np.load()`.
with np.load(&quot;/var/data/training_data.npy&quot;) as data:
  features = data[&quot;features&quot;]
  labels = data[&quot;labels&quot;]

 # Assume that each row of `features` corresponds to the same row as `labels`.
assert features.shape[0] == labels.shape[0]

features_placeholder = tf.placeholder(features.dtype, features.shape)
labels_placeholder = tf.placeholder(labels.dtype, labels.shape)

dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))
 # [Other transformations on `dataset`...]
dataset = ...
iterator = dataset.make_initializable_iterator()

sess.run(iterator.initializer, feed_dict={features_placeholder: features,
labels_placeholder: labels})  </code></pre><h3 id="读取TFRecord数据"><a href="#读取TFRecord数据" class="headerlink" title="读取TFRecord数据"></a>读取TFRecord数据</h3><p> 　　tf.data.TFRecordDataset类能使用一个或多个TFRecord作为输入管道  </p>
<pre><code> # Creates a dataset that reads all of the examples from two files.
filenames = [&quot;/var/data/file1.tfrecord&quot;, &quot;/var/data/file2.tfrecord&quot;]
dataset = tf.data.TFRecordDataset(filenames)  </code></pre><p>　　TFRecordDataset的初始化参数filenames能够是一个string、一个string的列表、或者一个string的tenser。如果有训练和验证两部分文件，可以使用tf.placeholder(tf.string)，在初始化迭代器的时候使用合适的文件名：  </p>
<pre><code>filenames = tf.placeholder(tf.string, shape=[None])
dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.map(...)  # Parse the record into tensors.
dataset = dataset.repeat()  # Repeat the input indefinitely.
dataset = dataset.batch(32)
iterator = dataset.make_initializable_iterator()

 # You can feed the initializer with the appropriate filenames for the current
 # phase of execution, e.g. training vs. validation.

 # Initialize `iterator` with training data.
training_filenames = [&quot;/var/data/file1.tfrecord&quot;, &quot;/var/data/file2.tfrecord&quot;]
sess.run(iterator.initializer, feed_dict={filenames: training_filenames})

 # Initialize `iterator` with validation data.
validation_filenames = [&quot;/var/data/validation1.tfrecord&quot;, ...]
sess.run(iterator.initializer, feed_dict={filenames: validation_filenames})</code></pre><h3 id="读取文本数据"><a href="#读取文本数据" class="headerlink" title="读取文本数据"></a>读取文本数据</h3><p>　　许多数据集分布在一个或者多个文本文件中，tf.data.TextLineDataset 提供了一个简单的方法从多个文本文件中读取行数据。只需提供一个或者多个文件名，TextLineDataset就能逐行产生这些文件的string格式的元素值。类似TFRecordDataset，TextLineDataset也能接受filenames以tensors的形式，因此能够通过tf.placeholder(tf.string)实现参数化。  </p>
<pre><code>filenames = [&quot;/var/data/file1.txt&quot;, &quot;/var/data/file2.txt&quot;]
dataset = tf.data.TextLineDataset(filenames)    </code></pre><p>　　TextLineDataset所提取的每一行数据可能并不理想，例如文件始于标题行，或者含有注释。这些行能被Dataset.skip()或者Dataset.filter()移除。要对每个文件单独的使用这些操作，可以使用Dataset.flat_map()来对每个文件创建嵌套的Dataset。  </p>
<pre><code>filenames = [&quot;/var/data/file1.txt&quot;, &quot;/var/data/file2.txt&quot;]

dataset = tf.data.Dataset.from_tensor_slices(filenames)

 # Use `Dataset.flat_map()` to transform each file as a separate nested dataset,
 # and then concatenate their contents sequentially into a single &quot;flat&quot; dataset.
 # * Skip the first line (header row).
 # * Filter out lines beginning with &quot;#&quot; (comments).
dataset = dataset.flat_map(
lambda filename: (
tf.data.TextLineDataset(filename)
.skip(1)
.filter(lambda line: tf.not_equal(tf.substr(line, 0, 1), &quot;#&quot;))))  </code></pre><p>　　完整例子请见： <a href="https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/get_started/regression/imports85.py" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/examples/get_started/regression/imports85.py</a></p>
<h1 id="使用Dataset-map-来处理数据"><a href="#使用Dataset-map-来处理数据" class="headerlink" title="使用Dataset.map()来处理数据"></a>使用Dataset.map()来处理数据</h1><p>　　Dataset.map(f)对输入Dataset中的每个元素应用所给的函数来生成新的Dataset。函数f输入tensor对象，该tensors表示输入dataset中单个元素，返回tensor对象，表示输出dataset中的单个元素。下面是几个常用例子：</p>
<h3 id="解析-protocol-buffer-messages"><a href="#解析-protocol-buffer-messages" class="headerlink" title="解析 protocol buffer messages"></a>解析 protocol buffer messages</h3><p>　　许多输入管道从TFRecord-format文件中提取protocol buffer messages，每条记录中包括多个features，输入管道通常将这些特征转化为tensors。  </p>
<pre><code> # Transforms a scalar string `example_proto` into a pair of a scalar string and
 # a scalar integer, representing an image and its label, respectively.
def _parse_function(example_proto):
  features = {&quot;image&quot;: tf.FixedLenFeature((), tf.string, default_value=&quot;&quot;),
  &quot;label&quot;: tf.FixedLenFeature((), tf.int32, default_value=0)}
  parsed_features = tf.parse_single_example(example_proto, features)
  return parsed_features[&quot;image&quot;], parsed_features[&quot;label&quot;]

 # Creates a dataset that reads all of the examples from two files, and extracts
 # the image and label features.
filenames = [&quot;/var/data/file1.tfrecord&quot;, &quot;/var/data/file2.tfrecord&quot;]
dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.map(_parse_function)</code></pre><h3 id="解析图像数据并且resize"><a href="#解析图像数据并且resize" class="headerlink" title="解析图像数据并且resize"></a>解析图像数据并且resize</h3><p>　　当使用图像数据训练神经网络时，将不同大小的图像归一化为相同大小是很有必要的。</p>
<pre><code>def _parse_function(filename, label):
  image_string = tf.read_file(filename)
  image_decoded = tf.image.decode_image(image_string)
  image_resized = tf.image.resize_images(image_decoded, [28, 28])
  return image_resized, label

# A vector of filenames.
filenames = tf.constant([&quot;/var/data/image1.jpg&quot;, &quot;/var/data/image2.jpg&quot;, ...])

　# `labels[i]` is the label for the image in `filenames[i].
labels = tf.constant([0, 37, ...])

dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))
dataset = dataset.map(_parse_function)  </code></pre><h3 id="通过tf-py-func-使用任意python逻辑"><a href="#通过tf-py-func-使用任意python逻辑" class="headerlink" title="通过tf.py_func()使用任意python逻辑"></a>通过tf.py_func()使用任意python逻辑</h3><p>　　由于性能的原因，尽量使用tensorflow操作来处理数据，但是某些时候使用外部python库也是有用的，通过在Dataset.map()中调用tf.py_func()。  </p>
<pre><code>import cv2
 # Use a custom OpenCV function to read the image, instead of the standard
 # TensorFlow `tf.read_file()` operation.
def _read_py_function(filename, label):
  image_decoded = cv2.imread(image_string, cv2.IMREAD_GRAYSCALE)
  return image_decoded, label

 # Use standard TensorFlow operations to resize the image to a fixed shape.
def _resize_function(image_decoded, label):
  image_decoded.set_shape([None, None, None])
  image_resized = tf.image.resize_images(image_decoded, [28, 28])
  return image_resized, label

filenames = [&quot;/var/data/image1.jpg&quot;, &quot;/var/data/image2.jpg&quot;, ...]
labels = [0, 37, 29, 1, ...]

dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))
dataset = dataset.map(
lambda filename, label: tuple(tf.py_func(
_read_py_function, [filename, label], [tf.uint8, label.dtype])))
dataset = dataset.map(_resize_function)  </code></pre><h1 id="dataset-elements批量"><a href="#dataset-elements批量" class="headerlink" title="dataset elements批量"></a>dataset elements批量</h1><h3 id="Simple-batching"><a href="#Simple-batching" class="headerlink" title="Simple batching"></a>Simple batching</h3><p>　　最简单的batching方式就是将dataset中n个连续的元素堆叠成单个元素，使用Dataset.batch()，与tf.stack()有相同的约束。应用于elements中的每一部分。　　</p>
<pre><code>inc_dataset = tf.data.Dataset.range(100)
dec_dataset = tf.data.Dataset.range(0, -100, -1)
dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))
batched_dataset = dataset.batch(4)

iterator = batched_dataset.make_one_shot_iterator()
next_element = iterator.get_next()

print(sess.run(next_element))  # ==&gt; ([0, 1, 2,   3],   [ 0, -1,  -2,  -3])
print(sess.run(next_element))  # ==&gt; ([4, 5, 6,   7],   [-4, -5,  -6,  -7])
print(sess.run(next_element))  # ==&gt; ([8, 9, 10, 11],   [-8, -9, -10, -11])　　</code></pre><h3 id="Batching-tensors-with-padding"><a href="#Batching-tensors-with-padding" class="headerlink" title="Batching tensors with padding"></a>Batching tensors with padding</h3><p>　　以上例子的tensors是大小相同的，然而许多工作的输入数据大小并不一样，为了解决这种情况，Dataset.padded_batch()能对不同size的tensor进行batch，需要指定需要进行pad的维度。  </p>
<pre><code>dataset = tf.data.Dataset.range(100)
dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))
dataset = dataset.padded_batch(4, padded_shapes=[None])

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

print(sess.run(next_element))  # ==&gt; [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]]
print(sess.run(next_element))  # ==&gt; [[4, 4, 4, 4, 0, 0, 0],
   #  [5, 5, 5, 5, 5, 0, 0],
   #  [6, 6, 6, 6, 6, 6, 0],
   #  [7, 7, 7, 7, 7, 7, 7]]　　</code></pre><p>　　Dataset.padded_batch()允许在不同的维度进行不同的pad，长度可以是可变的或者是固定的，padding value可以被重写，默认值为0.</p>
<h1 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h1><h3 id="Processing-multiple-epochs"><a href="#Processing-multiple-epochs" class="headerlink" title="Processing multiple epochs"></a>Processing multiple epochs</h3><p>　　Dataset API 提供了两种主要方式来处理相同数据的多次epoch。<br>　　在多次epoch中迭代整个数据集最简单的方式是Dataset.repeat() 。举个例子，创建一个dataset，对输入重复进行了十次epoch。  </p>
<pre><code>filenames = [&quot;/var/data/file1.tfrecord&quot;, &quot;/var/data/file2.tfrecord&quot;]
dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.map(...)
dataset = dataset.repeat(10)
dataset = dataset.batch(32)  </code></pre><p>　　应用Dataset.repeat()不输入参数时将会无限的重复输入，此外这个函数没有epoch结束和开始的信号。<br>　　如果希望在每个epoch结束的时候收到信号，可以用如下写法：</p>
<pre><code>filenames = [&quot;/var/data/file1.tfrecord&quot;, &quot;/var/data/file2.tfrecord&quot;]
dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.map(...)
dataset = dataset.batch(32)
iterator = dataset.make_initializable_iterator()
next_element = iterator.get_next()

 # Compute for 100 epochs.
for _ in range(100):
  sess.run(iterator.initializer)
  while True:
try:
  sess.run(next_element)
except tf.errors.OutOfRangeError:
  break

   # [Perform end-of-epoch calculations here.]</code></pre><h3 id="Randomly-shuffling-input-data"><a href="#Randomly-shuffling-input-data" class="headerlink" title="Randomly shuffling input data"></a>Randomly shuffling input data</h3><p>　　Dataset.shuffle()随机洗牌输入数据，使用和tf.RandomShuffleQueue类似的算法：它维持一个固定大小的buffer，每次从buffer中随机选择一个元素作为下一个元素。  </p>
<pre><code>filenames = [&quot;/var/data/file1.tfrecord&quot;, &quot;/var/data/file2.tfrecord&quot;]
dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.map(...)
dataset = dataset.shuffle(buffer_size=10000)
dataset = dataset.batch(32)
dataset = dataset.repeat()  </code></pre><h3 id="使用高级API"><a href="#使用高级API" class="headerlink" title="使用高级API"></a>使用高级API</h3><p>　　tf.train.MonitoredTrainingSession API简化了在分布式环境中运行tensorflow的许多方面，它使用tf.errors.OutOfRangeError 来表示训练完成，因此推荐与Dataset.make_one_shot_iterator()一起使用，例如：  </p>
<pre><code>filenames = [&quot;/var/data/file1.tfrecord&quot;, &quot;/var/data/file2.tfrecord&quot;]
dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.map(...)
dataset = dataset.shuffle(buffer_size=10000)
dataset = dataset.batch(32)
dataset = dataset.repeat(num_epochs)
iterator = dataset.make_one_shot_iterator()

next_example, next_label = iterator.get_next()
loss = model_function(next_example, next_label)

training_op = tf.train.AdagradOptimizer(...).minimize(loss)

with tf.train.MonitoredTrainingSession(...) as sess:
  while not sess.should_stop():
sess.run(training_op)  </code></pre><p>　　在tf.estimator.Estimator中使用Dataset作为input_fn时，也推荐使用Dataset.make_one_shot_iterator()，例如： </p>
<pre><code>def dataset_input_fn():
  filenames = [&quot;/var/data/file1.tfrecord&quot;, &quot;/var/data/file2.tfrecord&quot;]
  dataset = tf.data.TFRecordDataset(filenames)

  # Use `tf.parse_single_example()` to extract data from a `tf.Example`
  # protocol buffer, and perform any additional per-record preprocessing.
  def parser(record):
keys_to_features = {
&quot;image_data&quot;: tf.FixedLenFeature((), tf.string, default_value=&quot;&quot;),
&quot;date_time&quot;: tf.FixedLenFeature((), tf.int64, default_value=&quot;&quot;),
&quot;label&quot;: tf.FixedLenFeature((), tf.int64,
default_value=tf.zeros([], dtype=tf.int64)),
}
parsed = tf.parse_single_example(record, keys_to_features)

# Perform additional preprocessing on the parsed data.
image = tf.decode_jpeg(parsed[&quot;image_data&quot;])
image = tf.reshape(image, [299, 299, 1])
label = tf.cast(parsed[&quot;label&quot;], tf.int32)

return {&quot;image_data&quot;: image, &quot;date_time&quot;: parsed[&quot;date_time&quot;]}, label

  # Use `Dataset.map()` to build a pair of a feature dictionary and a label
  # tensor for each example.
  dataset = dataset.map(parser)
  dataset = dataset.shuffle(buffer_size=10000)
  dataset = dataset.batch(32)
  dataset = dataset.repeat(num_epochs)
  iterator = dataset.make_one_shot_iterator()

  # `features` is a dictionary in which each value is a batch of values for
  # that feature; `labels` is a batch of labels.
  features, labels = iterator.get_next()
  return features, labels</code></pre>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/tensorflow/" rel="tag"># tensorflow</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/10/24/tensorflow initializer种类/" rel="next" title="tensorflow initializer种类">
                <i class="fa fa-chevron-left"></i> tensorflow initializer种类
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/01/02/summary,tensorboard使用/" rel="prev" title="summary,tensorboard使用">
                summary,tensorboard使用 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">yvelzhang</p>
              <p class="site-description motion-element" itemprop="description">喵喵喵</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#importing-data"><span class="nav-number">1.</span> <span class="nav-text">importing data</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#基础教学"><span class="nav-number">2.</span> <span class="nav-text">基础教学</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Dataset的结构"><span class="nav-number">3.</span> <span class="nav-text">Dataset的结构</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#创建迭代器"><span class="nav-number">4.</span> <span class="nav-text">创建迭代器</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#从迭代器中获得值"><span class="nav-number">5.</span> <span class="nav-text">从迭代器中获得值</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#读取输入值"><span class="nav-number">6.</span> <span class="nav-text">读取输入值</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#读numpy数组"><span class="nav-number">6.0.1.</span> <span class="nav-text">读numpy数组</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#读取TFRecord数据"><span class="nav-number">6.0.2.</span> <span class="nav-text">读取TFRecord数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#读取文本数据"><span class="nav-number">6.0.3.</span> <span class="nav-text">读取文本数据</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#使用Dataset-map-来处理数据"><span class="nav-number">7.</span> <span class="nav-text">使用Dataset.map()来处理数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#解析-protocol-buffer-messages"><span class="nav-number">7.0.1.</span> <span class="nav-text">解析 protocol buffer messages</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#解析图像数据并且resize"><span class="nav-number">7.0.2.</span> <span class="nav-text">解析图像数据并且resize</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#通过tf-py-func-使用任意python逻辑"><span class="nav-number">7.0.3.</span> <span class="nav-text">通过tf.py_func()使用任意python逻辑</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dataset-elements批量"><span class="nav-number">8.</span> <span class="nav-text">dataset elements批量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Simple-batching"><span class="nav-number">8.0.1.</span> <span class="nav-text">Simple batching</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batching-tensors-with-padding"><span class="nav-number">8.0.2.</span> <span class="nav-text">Batching tensors with padding</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#训练过程"><span class="nav-number">9.</span> <span class="nav-text">训练过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Processing-multiple-epochs"><span class="nav-number">9.0.1.</span> <span class="nav-text">Processing multiple epochs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Randomly-shuffling-input-data"><span class="nav-number">9.0.2.</span> <span class="nav-text">Randomly shuffling input data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用高级API"><span class="nav-number">9.0.3.</span> <span class="nav-text">使用高级API</span></a></li></ol></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yvelzhang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.3</div>


<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style="display:none">
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次
    <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style="display:none">
    有<span id="busuanzi_value_site_uv"></span>人看过我的博客啦
</span>
</div>



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
